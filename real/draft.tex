%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Bayesian prediction of luminosity distribution based on one simulation run:
% An example from TARDIS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage[utf8]{inputenc}

\usepackage[a4paper, left=20mm, right=20mm, top=25mm, bottom=25mm]{geometry}
\usepackage[numbers]{natbib}
\usepackage{hyperref}
\usepackage{physics}

\usepackage[colorinlistoftodos]{todonotes} % Use 'disable' to remove todos in final version
\newcommand{\fred}[1]{\todo[color=orange!40,inline]{#1}} %
\newcommand{\fredmargin}[1]{\todo[color=orange!40]{#1}} %
\newcommand{\checked}{\todo[color=green,noline]{\checkmark}} %
\newcommand{\checkedl}{\todo[color=brown]{\checkmark}} %
\newcommand{\hans}[1]{\todo[color=yellow!30,inline]{#1}} %
\newcommand{\hanslong}[2][]{\todo[%bordercolor=red,
  color=white,inline,caption={2do}, #1]{
    \begin{minipage}{\textwidth} #2\end{minipage}}} %

%% CHANGE THIS DATE AS YOU MODIFY THE FILE
\newcommand{\vdate}{170308}
\pagestyle{myheadings}
\markboth{Version \vdate} {Version \vdate}

\parindent=3ex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm,amssymb,amsfonts,amsmath}  % bold math symbols, AMStex
\usepackage{graphicx}    % standard graphics
\graphicspath{
  {./figures/}
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\lleq}[1]{\label{#1} }
% TO REMOVE THE LABEL LETTERS FROM THE EQUATION DISPLAY, COMMENT OUT THIS LINE:
\renewcommand{\lleq}[1]{\label{#1} {\scriptstyle {\rm (#1)}} \hspace*{2ex} }

\input{symbols}

\newcommand{\hypo}  {{\mathcal{H}}}  % Model
\newcommand{\ldef}{\;{:}{=}\;}
\newcommand{\rdef}{\;{=}{:}\;}
\newcommand{\realnumbers}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\cond}{\,|\,}
\newcommand{\NBD}{\mathrm{NB}}

\newcommand{\refeq}[1]{Eq.~(\ref{#1})}
\newcommand{\reffig}[1]{Fig.~\ref{fig:#1}}
\newcommand{\refsec}[1]{Sec.~\ref{sec:#1}}
\DeclareMathOperator{\erfc}{erfc}
\DeclareMathOperator{\Expect}{\mathbb{E}}
\newcommand{\expect}[1]{\Expect\left[#1\right]}
\newcommand{\expectest}[1]{\widehat{\Expect\left[#1\right]}}
\DeclareMathOperator{\GammaDist}{Gamma}
\DeclareMathOperator{\GaussianDist}{\mathcal{N}}
\DeclareMathOperator{\InvGammaDist}{InvGamma}
\newcommand{\Kalpha}{{K_\alpha}}
\newcommand{\Kbeta}{{K_\beta}}
\newcommand{\npack}{{n_p}}
\newcommand{\lmax}{\ell_{\rm max}}
\newcommand{\Lmax}{{L_{\rm max}}}
\newcommand{\Lumtot}{Q}
\newcommand{\lumtot}{q}
\newcommand{\Lum}{L}
\newcommand{\lum}{\ell}
\newcommand{\rmdx}[1]{\dd{#1}} % differential
\newcommand{\firstDeriv}[1]{\frac{\partial}{\partial #1}}
\newcommand{\secDeriv}[1]{\frac{\partial^2}{\partial #1^2}} % second partial derivative
\newcommand{\secPartial}[2]{\frac{\partial^2}{\partial #1 \, \partial #2}} % second partial derivative
\newcommand{\tardis}{TARDIS}
\DeclareMathOperator{\Variance}{\mathbb{V}}
\newcommand{\variance}[1]{\Variance\left[#1\right]}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{center}
  \textbf{\Large Bayesian prediction of supernova luminosity  distributions}\\[8pt]
  \textbf{\Large based on simulation output}\\[12pt]
\end{center}

\textbf{Abstract} TODO\\

\section{Introduction (Wolfgang)}

\subsection{Astrophysical context}

Wolfgang's text

\subsection*{Main points of this paper}

\begin{enumerate}
\item The first goal of this paper is to show that the Bayesian
  approach to this problem is well-founded and robust.

\item Luminosity data generated by a single simulation provide a
  basis for predicting its uncertainty, i.e.\ how much this luminosity
  can be expected to vary from simulation to simulation.

\item The uncertainty in the luminosity calculated in the Bayesian
  approach is larger than anticipated because it takes into account
  not only the variability of the luminosity per packet, but also the
  distribution of the number of packets.

\item Traditional methods using conventional counting ratios
  (``relative frequencies'') can also make predictions for such
  uncertainties, but only if the number of packets is large. By
  contrast, the Bayesian method presented here remains valid also for
  small packet multiplicities.

\end{enumerate}

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.48\textwidth]{spectrum_bin}
  \includegraphics[width=0.48\textwidth]{spectrum_all}
  \caption{Spectrum with most likely value of $Q$ (dot) and smallest
    2$\sigma$ credibility interval (blue band). Left: only packets
    from single bin contribute to the single-packet luminosity
    distribution in that bin. If $n<8$ packets observed, no reliable
    prediction is available and bins are masked out (red band) Right:
    Same packets as on the left but the single-packet luminosity
    distribution is inferred from all packets. }
\label{fig:spectrum}
\end{figure}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Bayesian prediction}
\label{sec:bayp}

\fred{This is a quick summary of Bayesian inference, we could refer to some text books for more details}
We first sketch the generic notation and relations for Bayesian
probability theory.
%
A particular experiment or simulation generates
\textit{data}
which will often come in the form of a list of $n$ measurements,
$\bmy = (y_1,y_2,\ldots,y_n)$. In order to understand or explain these
data, the observer constructs one or more \textit{models} or
\textit{hypotheses} $\hypo$. Central to this construction is the
specification of a \textit{likelihood} $p(\bmy\cond \bmtheta,\hypo)$
where $\bmtheta = (\theta_1,\theta_2,\ldots,\theta_K)$ is the set of
unknown parameters specified by the model. The likelihood is the
probability assigned by the model $\hypo$ to the data $\bmy$ when the
parameters $\bmtheta$ are fixed and known.
%
If and when the individual measurements $y_i$ are independent, the
joint likelihood is the product
\begin{align}
  \lleq{bta}
  p(\bmy\cond \bmtheta) &= \prod_{i=1}^n p(y_i\cond \bmtheta).
\end{align}
At this level of analysis, the goal is to find which values for these
parameters provide a satisfactory description of the data. In the
Bayesian context, these parameter values are best described in terms
of the probability of the parameters given the data,
$p(\bmtheta\cond \bmy)$. This so-called \textit{posterior} is related
to the likelihood by Bayes' Theorem,
\begin{align}
  \lleq{bth}
  p(\bmtheta\cond \bmy)
  &= \frac{p(\bmy\cond\bmtheta)\;p(\bmtheta)}
    {p(\bmy)}
    %= \frac{p(\bmy\cond\bmtheta)\;p(\bmtheta)}
    %{\int \rmdx{\bmtheta}'\, p(\bmy\cond\bmtheta')\;p(\bmtheta')}
\end{align}
where $p(\bmtheta)$, the so-called \textit{prior}, is the probability
assigned to the values of the parameters before any data were taken
into account and the \textit{evidence}
\begin{align}
  \lleq{bti}
  p(\bmy) &= \int \rmdx{\bmtheta}'\, p(\bmy\cond\bmtheta')\;p(\bmtheta'),
\end{align}
is fully determined by the likelihood and prior.
%
Prediction of any ``future'' data $Y$ given the present data $\bmy$ is made not by considering the
likelihood $p(Y\cond\bmtheta_{\rm max})$ using single
maximum-likelihood parameter values $\bmtheta_{\rm max}$, but by
integrating over all possible values of the likelihood, weighted by
the posterior,
\begin{align}
  \lleq{btj}
  p(Y\cond \bmy) &= \int \rmdx{\bmtheta}\,p(Y\cond\bmtheta)\,p(\bmtheta\cond \bmy).
\end{align}
While in this paper, we apply the above relations to simulated data
only, they are valid also for actual experimental data and the
pertinent physical parameters.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Single-simulation data and multi-simulation prediction}

% \subsection{Output data from radiative transfer simulations}
% \subsection{Simulation data}

We consider a single computer simulation which generates a luminosity
spectrum from $\npack$ photon packets, each having a particular
frequency $\nu_i$ and luminosity $\lum_i$. Together, these frequencies
and luminosities form the raw simulation data
$\bmy = \{ (\nu_i,\lum_i): i = 1,\ldots,\npack\}$. Typically, the
frequency spectrum is partitioned into bins of width $\Delta\nu$
centered on frequencies $\nu_b$ resulting in bin intervals
$\Delta_b = [\nu_b {-} \tfrac{1}{2} \Delta\nu \,,\, \nu_b {+}
\tfrac{1}{2} \Delta\nu], b = 1,2,\ldots, B$.  Let $n_b$ be the number
of packets having a frequency falling into a particular bin $b$, and
let the vector $\bml_b = \{(\ell_{b,j}): j = 1,\ldots,n_b\}$ be the
set of luminosities of those packets. The data set is correspondingly
cast into the more compact histogram form
\begin{align}
  \lleq{sdb}
  \bmy = \{(n_b,\bml_b): b = 1,\ldots,B\}
\end{align}
with loss of information limited to replacing the exact packet
frequencies $\nu_i$ by the approximate bin frequency $\nu_b$. The
total luminosity in bin $b$ is
\begin{align}
  \lleq{sdc}
  \lumtot_b
  &= \sum_{j=1}^{n_b} \lum_{b,j}.
\end{align}
Within a given model $\hypo$, the data $\bmy$ are only one of many
possible realisations of the corresponding random variables; for
example, starting the simulation with a different seed would result in
different output data. Following standard notation, we denote the
general random variables by the corresponding upper-case symbols,
\begin{align}
  \lleq{sdd}
  \bmL_b &= \{(\Lum_{b,j}): j = 1,\ldots,N_b\}\\
  \lleq{sde}
  \bmY &= \{(N_b, \bmL_b): b = 1,\ldots,B\},\\
  \lleq{sdf}
  \Lumtot_b &= \sum_{j=1}^{N_b} \Lum_{b,j}.
\end{align}
Lower-case symbols such as $\bmy, n_b,\bml_b$ etc.\ are strictly
reserved for the realisations; i.e., the simulated data at hand.


The goal of this paper is to illustrate how the single-simulation data
$\bmy$ can be used to predict the distribution of total bin
luminosities $Q_b, b = 1,\ldots,B$ if many simulations had been
carried out. At this introductory level, we assume that the
luminosities at different frequencies are independent, so that the
prediction of the entire spectrum (including all its uncertainty) is
the product of predictions for each frequency bin,
\begin{align}
  \lleq{sdg}
  p(Q_1,Q_2,\ldots,Q_B\cond\bmy) = \prod_{b=1}^B p(Q_b\cond \bmy).
\end{align}

\section{The generic model} \label{sec:model}

Various physical processes such as absorption result in luminosity
spectra with strong features. The model we develop below takes into
account the variability introduced by these features in a two-step
process termed a \textit{compound process} in the statistics
literature.
%
In the first step, the number of packets $N$ is modelled by a Poisson
distribution $p(N\cond\lambda)$, separately for each bin, where the
change in magnitude of $\lambda$ from bin to bin will capture most of
the variability of the spectrum. Given $N$, the single-packet
luminosity distribution is modelled in the second step by means of a
packet likelihood for $\Lum_{b,j}$ with packet parameters, a packet
prior and, of course, a packet parameter posterior. The single-packet
distribution contains the crucial information which allows us to
extend previous
methods~\cite{bohm_comparison_2012,bohm_statistics_2014} to cases
where the number of packets in a bin is small and even zero.

We derive in this section the generic equations of the compound
process, and then proceed to show in Sections \ref{sec:example} and
\ref{sec:asymptotic} how these equations can be used in two different
ways.
%
We consider for the moment quantities in one bin $b$ only, and hence
drop the $b$-subscripts, e.g.\
$Q_b \to Q, n_b \to n, N_b\to N, \bml_b\to\bml, L_{b,j} \to L_j$ etc.


We seek a Bayesian prediction for the total luminosity in one bin,
$p(Q\cond n,\bml)$, one of the factors in Eq.~(\ref{sdg}).  To do so,
we must take into account all possible values of variables and
parameters entering our model.  This involves multiple use of the
\textit{marginalisation rule}, which states that, to obtain the
probability for a variable $U$ when we are not interested in another
variable $V$, the marginal probability for $U$ is %
$p(U) = \int dV\,p(U,V) = \int dV\,p(U\cond V)\,p(V)$ for continuous
$V$ and $p(U) = \sum_V p(U,V) = \sum_V p(U\cond V)\,p(V)$ for discrete
$V$ respectively.
%
The uninteresting variables $V$ in the present case are the number of
packets $N$, the set of individual luminosities $\bmL$ of the $N$ packets
whose likelihood is modelled by a distribution determined by
generic likelihood parameters $\bmphi$,
\begin{align}
  \lleq{slk}
  p(\bmL\cond N,\bmphi)
  &= \prod_{j=1}^{N} p(L_j\cond \bmphi)
\end{align}
and the parameter $\lambda$ governing the likelihood for the number of
packets, $p(N\cond\lambda)$. Multiple use of the marginalisation rule
on $p(Q\cond n,\bml)$ results in
\begin{align}
  \lleq{sll}
  p(Q\cond n,\bml)
  &= \sum_{N=0}^\infty \;\int_0^\infty \rmdx{\lambda}\;
    p(N\cond\lambda)\;
    p(\lambda\cond n)
    \int \rmdx{\bmL}\;
    p(Q\cond N,\bmL)\;
    \int \rmdx{\bmphi}\;
    p(\bmL\cond N,\bmphi)\,
    p(\bmphi\cond n,\bml),
\end{align}
where we have omitted from the probabilities all quantities to the
right of the conditional line which are irrelevant to the variable at
hand. For example, Eq.~(\ref{sdf}) shows that the total bin luminosity
$Q$ is fully determined by $N$ and $\bmL$ and so we can shorten
$p(Q\cond N,\bmL,\lambda,\bmphi,n,\bml)$ to $p(Q\cond N,\bmL)$. %
Likewise the probability for $\bmL$ is fully determined by $N$ and the
parameters $\bmphi$ and does not depend on the data $n,\bml$.  The
bounds of $\int \rmdx{\bmL}$ and $\int \rmdx{\bmphi}$ are determined by the
specific model and data for which $p(Q\cond n,\bml)$ is to be
calculated.
%
We note that a part of Eq.~(\ref{sll}) can be seen as a convolution
since $p(Q\cond N,\bmL)$ can be written as a Dirac delta function,
\begin{align}
  \lleq{slm}
  \int \rmdx{\bmL}\; p(Q\cond N,\bmL)\; p(\bmL\cond N,\bmphi)
  &= \int \rmdx{\bmL}\;\delta(Q - \textstyle\sum_{j=1}^N L_j)\; \prod_{j=1}^N p(L_j\cond \bmphi).
\end{align}
Eq.~(\ref{sll}) is exact, and in principle all that remains is to make
specific choices for the various probabilities which are judged best
to represent the problem at hand, and thence to carry out the sum and
multiple integrals. In practice, of course, such calculations may be
time-consuming, and there will often be more than one choice for the
functional form of each probability which may critically affect the
computational effort if \refeq{sll} has to be repeatedly evaluated for
thousands of bins.

\section{Example: modelling single-packet luminosity distributions}
\label{sec:example}

In this section, we show by example how information on the
single-packet luminosity distributions can be utilised to find exact
expressions for Eq.~(\ref{sll}). Our choices are motivated by our
original application \tardis{} but we emphasize that much of the
analysis below, especially \refsec{multiplicity}, is generally
applicable.

\subsection{Multiplicity prediction} \label{sec:multiplicity}

First, we eliminate the parameter $\lambda$ of the Poisson
multiplicity distribution for the number of packets $N$,
\begin{align}
  \lleq{pmb}
  p(N\cond\lambda) &= \frac{e^{-\lambda}\lambda^{N}}{N!}
  \quad N = 0,1,\ldots,\infty.
\end{align}
The simulation number of packets $n$ is naturally presumed to have
arisen from the same Poisson distribution with the same $\lambda$.
The posterior for $\lambda$ is found from the data $n$ via Bayes'
Theorem,
\begin{align}
  \lleq{pmc}
  p(\lambda\cond n) %
  &= \frac{p(n\cond\lambda)\,p(\lambda)} {p(n)}
  \ =\ \frac{p(n\cond\lambda)\,p(\lambda)} %
  {\int \rmdx{\lambda} p(n\cond\lambda)\,p(\lambda)} \,.
\end{align}
We use a prior
\begin{align}
  \lleq{pmf}
  p(\lambda) = c \lambda^{-a}
\end{align}
which is ``improper'', meaning that
$\int_0^\infty \rmdx{\lambda}\,p(\lambda)$ is known only up to a
constant $c$.\footnote{For a uniform prior, the case $a{=}0$, we
  additionally require an upper limit $\lambda_{\rm max}$. As long as
  $\lambda_{\rm max}$ is large enough, it would also cancel.}, The
posterior is
\begin{align}
  \lleq{pms}
  p(\lambda\cond n)
  &= \GammaDist(\lambda \cond \alpha = n{-}a{+}1 \,,\, \beta = 1)\,,
\end{align}
where the $\GammaDist$ distribution with shape parameter $\alpha > 0$ and
rate parameter $\beta > 0$ is defined as
\begin{align}
  \lleq{gma}
  \GammaDist(x\cond \alpha, \beta)
  &= \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x}\,,
  \quad 0 \leq x < \infty.
\end{align}
The evidence is
$p(n) =  \int_0^\infty \rmdx{\lambda}\, p(n\cond\lambda)\,p(\lambda) = c\,(n-a)!/n!$.
% \begin{align}
%   \lleq{pmg}
%   p(n)
%   &=  \int_0^\infty \rmdx{\lambda}\, p(n\cond\lambda)\,p(\lambda)
%     \ =\ c\frac{(n-a)!}{n!} \,, \\
% \end{align}
Upon inserting Eqs.~(\ref{pmb}) and (\ref{pmc}), $c$ cancels out and
the integral over $\lambda$ can be performed analytically resulting in
a negative binomial distribution that is proper for any $n$
\begin{align}
  \lleq{pmh}
  p(N\cond n) %
  &= \int_0^\infty \rmdx{\lambda}\, p(N\cond\lambda)\,p(\lambda\cond n)
  \ =\ \binom{N{+}n{-}a}{N} \left(\frac{1}{2}\right)^{N+n-a+1}\, .
    %\ =\ \frac{(N+n-a)!}{N!\,(n-a)!\,2^{N+n+1-a}} \, .
\end{align}
A hyperparameter choice $a=0$ yields the uniform prior, $a=1/2$ gives
the Jeffreys and reference prior, and $a=1$ is the
transformation-group result advocated by
Jaynes~\cite[Ch. 12]{jaynes2003probability}. To decide on $a$, we note
that the uniform prior has no solid information-theoretic foundation
in this case reducing the choice to $a \in \{1/2, 1\}$. We examine the
consequence for the extreme case that no packet is observed:
$P(N=0 \cond n=0) = 2^{a-1}$. For $a=1$, any $N>0$ is excluded which
is undesirable unless we accept that repeating the simulation will
never yield a packet in the current bin. Hence we settle on Jeffreys
prior, $a=1/2$, for any numerical evaluation carried out below.

With \refeq{pmh}, \refeq{sll} simplifies to
\begin{align}
  \lleq{gmh}
  p(Q\cond n,\bml)
  &= \sum_{N=0}^\infty \;
    p(N\cond n)\;
    \int \rmdx{\bmL}\;
    p(Q\cond N,\bmL)\;
    \int \rmdx{\bmphi}\;
    p(\bmL\cond N,\bmphi)\,
    p(\bmphi\cond n,\bml).
\end{align}

\subsection{Single-packet description} \label{sec:single-packet}

Next, we consider detailed modelling of $p(\bmL\cond N,\bmphi)$ which
was written in Eq.~(\ref{slk}) as a product of single-packet
distributions $p(L_j\cond \bmphi)$, meaning that single packets are
assumed to be mutually independent and to follow the same distribution
independent of $N$ and the exact frequency within the bin under consideration.
%
Which specific probability function to choose for $p(L_j\cond\bmphi)$
depends, of course, on the specific simulation data. In this section,
we choose it to be the same Gamma distribution for all $L_j$
with parameters $\bmphi = (\alpha,\beta)$,
\begin{align}
  \lleq{spc}
  p(L_j\cond \bmphi) &= \GammaDist(L_j\cond \alpha,\beta)
\end{align}
which has the nice property that the convolution \refeq{slm} for
$N > 0$ results in another Gamma distribution,
\begin{align}
  \lleq{spd}
  \int \rmdx{\bmL}\; p(Q\cond N,\bmL)\; p(\bmL\cond N,\bmphi)
  &= \GammaDist(Q\cond N\alpha, \beta).
\end{align}
For the special case $N=0$, there is no luminosity because there is no
packet, so we define
\begin{align}
  \lleq{spl}
   p(Q\cond N, \alpha, \beta) &\equiv
   \begin{cases}
     \delta(Q),     &N=0,\\
     \GammaDist(Q\cond N\alpha, \beta), & N > 0 \, ,
   \end{cases}
 \end{align}
 with the Dirac $\delta$ function and so \refeq{gmh} simplifies to
\begin{align}
  \lleq{spe}
  p(Q\cond n,\bml)
  &= \sum_{N=0}^\infty \;
    p(N\cond n)\;
    \int \rmdx{\alpha}\,\rmdx{\beta}\;
    p(Q\cond N, \alpha, \beta)\;
    % \GammaDist(Q\cond N\alpha, \beta)\;
    p(\alpha,\beta\cond n,\bml).
\end{align}
The posterior for the parameters is calculated by yet another
application of Bayes' Theorem,
\begin{align}
  \lleq{spf}
  p(\alpha,\beta\cond n,\bml)
  &= \frac{p(n,\bml\cond \alpha,\beta)\,p(\alpha,\beta)}{p(n,\bml)}
\end{align}
where the data $(n,\bml)$ must naturally follow the same
Gamma likelihood as $(N,\bmL)$, i.e.\
\begin{align}
  \lleq{spg}
  p(n,\bml\cond \alpha,\beta)
  & = \prod_{j=1}^n \GammaDist(\ell_j\cond\alpha,\beta)
    \ =\ \frac{\beta^{n\alpha} r^{\alpha-1}\,e^{-\beta q}}{[\Gamma(\alpha)]^n}
\end{align}
with the data summarized by the sufficient statistics $n$,
$q \equiv \sum_{j=1}^n \ell_j$, and $r \equiv \prod_{j=1}^n \ell_j$.

It only remains to specify the prior $p(\alpha,\beta)$ appearing in
\refeq{spf}.
%
The authors of \cite{moala2013bayesian} present an overview of
possible choices indicating that their \emph{modified MDIP} prior
performed best in the few examples considered there. We do not follow
their recommendation because their modification is not well motivated
and it seems to only accidentally perform better because of the
particular examples they considered. The other conclusion they have is
that the Jeffreys, reference, and other noninformative priors with
similarly solid motivation have comparable performance. They all
exhibit the standard form for a scale or rate parameter
$\propto 1/\beta$ and only differ in the $\alpha$ dependence. We
disregard the reference prior as it forces us to identify either
$\alpha$ or $\beta$ as \emph{the} nuisance parameter but we consider
\emph{both} as nuisance parameters. Hence we select the Jeffreys prior
\begin{align}
  \lleq{sph}
  p(\alpha,\beta)
  &=  C \frac{\sqrt{\alpha \Psi'(\alpha)-1}}{\beta} \, ,
\end{align}
where $\Psi$ is the digamma function. With these choices, the
posterior is nondegenerate for $n \geq 2$ and the evidence is
\begin{align}
  \lleq{spj}
  p(n, \bml) &= C \int_0^{\infty} \rmdx{\alpha} \int_0^{\infty} \rmdx{\beta}\;
  \frac{\beta^{n\alpha} r^{\alpha-1}\,e^{-\beta q}}{[\Gamma(\alpha)]^n}\;
  \frac{\sqrt{\alpha \Psi'(\alpha)-1}}{\beta}.
\end{align}
The prior's constant of proportionality $C$ cancels in the final
result and we may set $C\equiv 1$ to obtain
\begin{align}
  \lleq{spk}
  p(Q\cond n,\bml)
  &= \frac{1}{p(n, \bml)}
    \sum_{N=1}^\infty \;
    p(N\cond n)\;
    \int \rmdx{\alpha}\,\rmdx{\beta}\;
    \GammaDist(Q\cond N\alpha, \beta)\;
    \frac{\beta^{n\alpha} r^{\alpha-1}\,e^{-\beta q}}{[\Gamma(\alpha)]^n} \frac{\sqrt{\alpha \Psi'(\alpha)-1}}{\beta}
\end{align}
for $Q > 0$ and for $Q=0$ there is the additional contribution from
$N=0$ through the $\delta$ function from \refeq{spl}.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Large packet number approximations} \label{sec:asymptotic}

In practice, the situation may arise that many packets fall into a
bin. One can then avoid modeling the luminosity distribution
explicitly as done in \refsec{single-packet} as it becomes irrelevant
in the limit $n \to \infty$, essentially due to the central limit
theorem. As a bonus, the numerical effort to evaluate the basic
\refeq{sll} simplifies drastically.

\subsection{Derivation of $p(Q\,|\,n,\boldmath{\ell})$ for large $n$}

We return to the general formulation of \refeq{sll} and re-arrange its
factors into
\begin{align}
  \lleq{asc}
  p(Q\cond n,\bml)
  &= \int \rmdx{\lambda}\; \rmdx{\bmphi}\;
    p(\lambda\cond n)\;
    p(\bmphi\cond n,\bml)\;
    p(Q\cond \lambda,\bmphi), \\
  \lleq{asd}
  p(Q\cond \lambda,\bmphi)
  &= \sum_N
    p(N\cond \lambda)\;
    \int \rmdx{\bmL}\;
    p(Q\cond N,\bmL)\;
    p(\bmL\cond N,\bmphi),
\end{align}
with $p(\bmL\cond N,\bmphi) = \prod_{j=1}^N p(L_j\cond\bmphi)$ as
before. Again all quantities are related to a single bin in frequency.
Writing the moments and variance of $L$ as\footnote{The $k$-th moment
  of variable $x$ is defined as the expectation value $\mu_k =
  \expect{x^k} = \int \rmdx{x}\,x^k\,p(x),\; k = 0,1,2,\ldots$.} %
\begin{align}
  \lleq{ram}
  \mu =  \mu_1 &\equiv \expect{\Lum}, \qquad
  \mu_2 \equiv \expect{\Lum^2}, \\
  \sigma^2 &\equiv \variance{\Lum}
  = \expect{\Lum^2} - \expect{\Lum}^2,
\end{align}
it is easy to show that, for the Poissonian compound process
(\ref{asd}), the first moment and variance of $Q$ are related to those
of $L$ by
\begin{align}
  \lleq{raa}
  \expect{Q} = \lambda \mu, \qquad
  \variance{Q} = \lambda \mu_2 = \lambda (\mu^2 + \sigma^2) \,.
\end{align}
for any density $p(L\cond\bmphi)$.
%
Higher-order quantities such as the skewness and kurtosis\footnote{
  The variance is the second so-called \textit{cumulant}. The relation
  for all orders is that the $k$-th order cumulant $\kappa_q(Q)$ is
  equal to $\lambda$ times the $k$-th order moment of $L$,
  $\mu_k$. The skewness of $p(Q\cond \lambda,\bmphi)$ is therefore
  $\kappa_3(Q)/\kappa_2(Q)^{3/2} = \lambda \mu_3/(\lambda \mu_2)^{3/2}
  = \lambda^{-1/2} \mu_3/\mu_2^{3/2}$.}  %
are suppressed by powers of $\lambda^{-1/2}$.
%%%%%%%%%%%
\hans{H comment (not for text): The magnitude of the correction to
  Eq.~(\ref{rab}) due to $\kappa_3$ and higher orders is unclear.}
%%%%%%%%%%%
From Theorem 4.3.1 of Ref.~\cite{bening2002generalized}, we conclude
that the asymptotic distribution of $\Lumtot$ when
$\lambda \to \infty$, given the finite first and second moment of $\Lum$, is
a Gaussian or normal distribution,
\begin{align}
  \lleq{rab}
  p(\Lumtot \cond \lambda,\bmphi)
  &= \GaussianDist \left( \Lumtot \cond \expect{Q}, \variance{Q} \right) %
    = \GaussianDist \left( \Lumtot \cond \lambda \mu, \lambda (\mu^2 {+} \sigma^2) \right).
\end{align}
It is remarkable that this result eliminates both the sum over $N$ and
the integral over $\bmL$ in \refeq{asd}.  What remains is to find the
integrals over $\lambda$ and $\bmphi$ in \refeq{asc}. As before, we
assume the posterior for $\lambda$ is the Gamma distribution of
\refeq{pms} since this is independent of $p(L\cond\bmphi)$.
%
To determine $p(\bmphi\cond n,\bml)$, we note that the result
(\ref{rab}) also simplifies the calculation of the parameter posterior
$p(\bmphi\cond n,\bml)$ since, independently of the actual functional
model for $p(L\cond\bmphi)$ the only relevant information is contained
in the first moment $\mu$ and variance $\sigma^2$. The maximum-entropy
distribution for $\Lum$ given knowledge of only $\mu$ and $\sigma^2$
is a Gaussian distribution, and its parameters are just $\mu$ and
$\sigma^2$ if the contribution from $\Lum < 0$ is negligible,
which we
may safely assume in the asymptotic regime, so we set
%
\fred{I checked this myself and clarified \href{http://stats.stackexchange.com/questions/83069/what-is-the-maximum-entropy-probability-density-function-for-a-positive-continuo/265763}{this unclear answer} on stats.stackexchange }
%
\begin{align}
  \lleq{rae}
  p(\Lum \cond \bmphi) =
  p(\Lum \cond \mu, \sigma^2 ) &= \GaussianDist(\Lum \cond \mu, \sigma^2)
\end{align}
and $\bmphi =(\mu,\sigma^2)$ and the posterior
becomes, using Bayes' Theorem,
\begin{align}
  \lleq{rac}
  p(\mu,\sigma^2\cond n,\bml)
  &= \frac{
    \GaussianDist(\bml\cond\mu,\sigma^2)\;
    p(\mu\cond n)\;
    p(\sigma^2\cond n)}
  {p(\bml\cond n)} \, .
\end{align}
In keeping with prior information that luminosities are always
positive, we assign an improper uniform prior
$p(\mu\cond\mu_{\rm max}) = 1/\mu_{\rm max}$ independent of $n$; as
always, the posterior is independent of $\mu_{\rm max}$.  For
$\sigma^2$, we use an $n$-independent Inverse Gamma prior with
hyperparameters $(a_0,b_0)$ which reads
\begin{align}
  \lleq{rad}
  p(\sigma^2\cond a_0,b_0)
  = \InvGammaDist(\sigma^2\cond a_0,b_0)
  &= \frac{b_0^{a_0}}{\Gamma(a_0)} (\sigma^2)^{-a_0-1} \exp(-b_0/\sigma^2)
\end{align}
and is ``conjugate'' to the normal distribution likelihood of
\refeq{rae} in that the posterior for $(\mu,\sigma^2)$ is also a
Gaussian in $\mu$ and an Inverse Gamma in $\sigma^2$, but with
parameters updated by the data,
\begin{align}
  \lleq{rbd}
  p(\mu,\sigma^2\cond n,\bml,a_0,b_0)
  &= \GaussianDist(\mu\cond \mu_n,\sigma_n^2)
  \cdot
  \InvGammaDist(\sigma^2\cond a_n,b_n) \\
  \mu_n &= \langle\ell\rangle\\
  \sigma_n^2 &= \sigma^2/n \\
  a_n &= a_0 + \tfrac{1}{2}n \\
  b_n &= b_0 + \tfrac{1}{2} n\left( \langle\ell^2\rangle - \langle\ell\rangle^2\right)
\end{align}
with data sample moments $\langle\lum^k\rangle$ defined as
$(1/n) \sum_{j=1}^n \lum_j^k$. Since we assume $n \gg 1$, we may
choose $a_0 = b_0 = 0$ resulting in a noninformative prior on $\sigma^2$.
%
Inserting Eqs.~(\ref{pms}) and (\ref{rbd}) into \refeq{asc}, we obtain
\begin{align}
  \lleq{ral}
  p(\Lumtot \cond n,\bml)
  &= \int \rmdx{\lambda}\; \rmdx{\mu}\; \rmdx{\sigma^2}\;
  \GaussianDist \left( \Lumtot \cond \lambda \mu, \lambda (\sigma^2{+}\mu^2) \right)
  \times\\
  &\quad\times
  \GammaDist(\lambda \cond n{-}a{+}1 , 1) \cdot
  \GaussianDist(\mu\cond \mu_n,\sigma_n^2) \cdot
  \InvGammaDist(\sigma^2\cond a_n,b_n) \;
  \nonumber
\end{align}

\subsection{Numerical implementation}\label{sec:asympt-numeric}


\begin{figure}[ht]
  \centering
  \includegraphics[width=0.48\textwidth]{asymptotic}
  \caption{Numerical prediction of $\Lumtot$ with the sum over $N$
    from \refeq{spk} or the asymptotic expression from \refeq{ral} for $n$
    samples from $\GammaDist(\alpha=1.5, \beta=60)$}
\label{fig:asymptotic}
\end{figure}

In \reffig{asymptotic}, we compare the asymptotic expression
\refeq{ral} with the ``exact'' expression \refeq{spk} where the
integrals over $\bmphi$ are done numerically to a requested relative
accuracy of $10^{-5}$ through the Cubature
package\cite{cubature,genz1980remarks,berntsen1991adaptive}.

For large $n$ where the asymptotic expression \refeq{ral} is valid,
asymptotic normality justifies the use of the Laplace or saddle-point
approximation~\cite[Ch. 27]{mackay2003information}. We implemented
\fredmargin{\href{https://github.com/tardis-sn/XXX}{github}}
the
above formulae in the julia language~\cite{julia14} and used Newton's
method from the \texttt{Optim.jl} package in combination with
automatic differentiation~\cite{RevelsLubinPapamarkou2016} to optimize
the respective integrand and to compute the Hessian at the mode as
required for the Laplace method. As initial values for optimization,
we slightly offset the sample estimates
\begin{align}
  \lleq{rat}
  \qty(\lambda, \mu, \sigma^2) = 1.0005 (n, \langle\lum\rangle, \langle\lum^2\rangle{-}\langle\lum\rangle^2)
\end{align}
to avoid issues if the optimizer cannot improve on the initial guess
and found convergence within $\order{10}$ steps.

As a test case, we generated
$\ell_i \sim \GammaDist(\alpha=1.5, \beta=60), i=1 \dots n$, and
computed $P(\Lumtot \cond n, \bml)$ on a grid of $\Lumtot$ values and
checked if the probability integrates to one using Simpson's
rule. With the exact expression, we obtain 0.997 ($n=10$) and 0.996
($n=80$), and 0.698 and 0.953 for the asymptotic expression. For
$n=10$, the large discrepancy arises mostly because the positivity
constraint $P(Q \leq 0 \cond n, \bml) = 0$ is not respected in
\refeq{ral} since the Gaussian approximation \refeq{rae} allows
negative $\lambda$ and $\Lumtot$, whereas the results \refeq{spk}
based on the $\GammaDist$ model imply $\lambda \geq 0$ and $Q \geq 0$.
%
\fred{Update values after Jeffreys implementing prior}

For that reason, we discourage the asymptotic approximation for small
$n$ but we note that in general, the lack of numerical normalization
may not be a concern. For example, if \refeq{ral} is used as the
likelihood for a single bin in Bayesian parameter inference, the
normalization or evidence is
irrelevant. Following~\cite{tierney_accurate_1986}, we renormalized
the results to integrate to one for ease of comparison in
\reffig{asymptotic}.  We verified that for $n>80$, the two methods
agree even better than shown there and that the normalization of the
asymptotic result approaches one. Integrating the asymptotic
expression with cubature instead of Laplace gave very similar results
but turns out to be less reliable and requires many more iterations.

The main advantage of the approximation is computational efficiency. A
single evaluation of $P(Q \cond n, \bml)$ with the Laplace methods
requires only about 100 calls to the integrand of \refeq{ral}
regardless of the value of $n$. In contrast, the numerical integration
of a single term for fixed $N$ in \refeq{spk} requires 1000s of
function evaluations, and the number of terms grows like $\sqrt{n}$
for a given accuracy of $P(Q \cond n, \bml)$.

\section{Practical considerations} \label{sec:prac-consid}

In this section, we use \tardis{} as an example to demonstrate both how
transforming the output of a simulation helps in predictions, and what
further complications may arise in practice.

\subsection{Preprocessing simulation output } \label{sec:tardis}

\tardis{} can output two kinds of packets, real and virtual. Here we
focus on the real packets and denote the raw output of \tardis{} by
primed symbols, for example for the $i$-{th} packet
$\qty(\nu_i^\prime, \ell_i^\prime)$.  A histogram plot of the
luminosities from a \tardis{} run indicates a strongly skewed
distribution with its single mode close to the maximum luminosity
$\lmax^\prime$, while there is a long tail for small $\ell$; see
\reffig{tardis}. These features can be accommodated by a $\GammaDist$
distribution as in \refsec{single-packet} if we transform the
luminosities to
\begin{align}
  \lleq{gmf}
  \ell_i \equiv 1- \frac{\ell_i^\prime}{\lmax^\prime} > 0
\end{align}
and similarly $L_i \equiv 1 - (L_i^\prime/\lmax^\prime)$. We assume
the total number of samples across all bins is large, so that the effect
of considering $\lmax^\prime$ a constant independent of the data is
negligible. We checked that leaving $\lmax^\prime$ as a free parameter
in a fit gave equivalent results.

\begin{figure}[ht]
  \centering
  \includegraphics[width=0.7\textwidth]{tardis_input_trafo}
  \caption{Histogram of 1000 packet luminosities in one frequency bin
    from \tardis{} before (left) and after (right) transforming. The
    maximum-likelihood fit of $\GammaDist(\alpha, \beta)$ is overlaid.}
\label{fig:tardis}
\end{figure}

Numerically, we take the largest luminosity of all \tardis{} packets
across the entire frequency range and increase it slightly to avoid
numerical difficulties at the end point
\begin{align}
  \lleq{gme}
  \lmax^\prime \equiv (1+\epsilon) \times \max_i \ell_i^\prime,
\end{align}
where $ \epsilon=10^{-6}$. We carry out all calculations with
$p(\Lumtot \cond n,\bml)$ and only in the end transform back to a
probability density in the raw coordinates via
\begin{align}
  \lleq{gmg}
  p(\Lumtot^\prime  \cond n, \bml^\prime)
  &= p(\Lumtot  \cond n, \bml) \frac{1}{\lmax^\prime} \,.
\end{align}
While we also rescale the frequencies by a constant from
$\mathcal{O}(10^{15})$ to $\mathcal{O}(1)$, the most important
numerical simplification arises because of the transformation of the
luminosities. Rewriting \refeq{sll} formally as
\begin{align}
  \lleq{gmhb}
  p(Q  \cond n,\bml)
  & = \sum_{N} p(N \cond n)\; p(Q \cond N, n, \bml),
\end{align}
we see that the right-hand side is a mixture density. In the \tardis{}
application, the distribution of a single-packet luminosity,
$p\qty(\ell_i' \cond \bmphi)$ is narrow in the sense that the standard
deviation relative to the mean is at the percent level. This implies
that each $p(Q^\prime \cond N, n, \bml^\prime)$ contributes one narrow
peak and all peaks are clearly separated in the superposition
$ p(Q^\prime \cond n,\bml^\prime)$. The transformation
$\ell^\prime \to \ell$ effects a strong overlap between the
distributions $p(\Lumtot \cond N, n, \bml)$ with similar $N$, so
$p(\Lumtot \cond n, \bml)$ can be well approximated by a unimodal
distribution and thus $\Lumtot$ rather than $\Lumtot^\prime$ is the
preferred coordinate basis.

\subsection{Towards an unbinned analysis} \label{sec:unbinned}

The parameters $\qty(\alpha, \beta)$ of the single-packet distribution
$\GammaDist(\ell_i \cond \alpha, \beta)$ may, of course, depend on the
frequency. % $\nu_i$
This effect could be modelled, for example, by a functional
parametrization $\alpha = \alpha(\nu \cond \bmphi)$ and
$\beta = \beta(\nu \cond \bmphi)$ in dependence on the frequency $\nu$
and on hyperparameters $\bmphi$ such that integrals over
$\alpha, \beta$ would turn into integrals over $\bmphi$. For the sake
of clarity, let us consider only the simplest case where
$\alpha, \beta$ are the same in every bin such that \refeq{spe}
becomes
\begin{align}
  \lleq{gmi}
  p(Q\cond \npack,\bml)
  &=     \sum_{N=0}^\infty \;
  p(N\cond n)\;
  \int \rmdx{\alpha}\,\rmdx{\beta}\;
  p(\Lumtot \cond N, \alpha, \beta)\;
  p(\alpha, \beta \cond \npack, \bmy) \, .
\end{align}
%   If it assumed that $p(L_j\cond \bmphi)$ does not change across
% \emph{different} bins, we may include packets from \emph{all} bins to
% further constrain $\alpha, \beta$ and the only required change is to
% set $n \to \npack$ in \refeq{spg}. Note that we would still keep $n$
% as before in $p(N \cond n)$ or else we would squash the entire
% spectrum into one bin.


In words, the posterior $ p(\alpha, \beta \cond \npack, \bmy)$ uses
all $\npack$ rather than $n$ packets to determine $\alpha, \beta$. For
$\npack \gg 1$, this allows meaningful predictions of $\Lumtot$ even
in bins where \emph{no} packet is observed; i.e., $n=0$. The only
binning left is performed to count $n$ the number of packets that fall
into a given frequency bin. This strategy is applicable, for example,
when absorption or emission lines in the spectrum abruptly alter the
number of packets for nearby frequencies but the luminosities are
unaffected by those line features. Note that if the dependence on
$\bmphi$ can be modeled with few parameters, the total number of
parameters may be drastically reduced: instead of determining
$\alpha, \beta$ separately for each of a few thousand bins, there are
potentially only a handful of elements in $\bmphi$ for the entire
spectrum. In that case, the integral in \refeq{gmi} does not depend on
the bin but only on $N$ so its value may be cached to speed up the
simultaneous evaluation across all bins.

Let's consider the extreme case $\npack \to \infty$ so we can neglect
the uncertainty on $\alpha, \beta$ and
$p(\alpha, \beta \cond \npack, \bml) =
\delta(\alpha-\alpha_0)\delta(\beta - \beta_0)$ such that
\begin{align}
  \lleq{gmj}
  p(Q\cond n, \alpha_0, \beta_0)
  &=     \sum_{N=0}^\infty p(N\cond n) p(Q\cond N, \alpha_0, \beta_0) \, .
\end{align}
For illustration only, let us consider the Poisson rate $\lambda$ were
known. Then learning $n$ would not improve our knowledge of $\lambda$
and we would have
\begin{align}
  \lleq{gmk}
  p(Q\cond \lambda, \alpha_0, \beta_0)
  &=     \sum_{N=0}^\infty p(N\cond \lambda) p(Q\cond N, \alpha_0, \beta_0) \, ,
\end{align}
where $p(N \cond \lambda)$ is the Poisson distribution \refeq{pmb}.
In \reffig{comp-unc}, we illustrate the impact of the uncertainty due
to $\lambda$ or $\alpha, \beta$ for $n=0,2, 20$ packets and continue
to use as true values $\alpha_0 = 1.5, \beta_0 = 60$. For $n>0$, we
set $\lambda=n$ as the mode of the posterior $p(\lambda \cond n)$, and
for $n=0$, we set $\lambda = 0.347$ by solving
\begin{align}
  \lleq{gml}
  p(N=0 \cond \lambda) &= p(N=0 \cond n=0)
\end{align}
such that the contribution at $N=0$ is identical for the Poisson
likelihood and Poisson posterior predictive distribution.

It is interesting to note that the Jeffreys prior for $\alpha, \beta$
leads to an extra mode at $\Lumtot = 0$ visible in the case $n=2$. We
checked that this is true for every individual term in the
\refeq{gmhb}, and the superposition of $p(\Lumtot \cond N, n, \bml)$
for various $N$ further enhances this effect. The reason for the
boundary mode is that $\GammaDist(\Lumtot \cond \alpha, \beta)$ peaks
at the boundary for $\alpha<1$, and peaks away from $\Lumtot=0$ for
$\alpha>1$. As $n$ increases, the likelihood concentrates around
$\alpha=1.5$, hence the distribution becomes unimodal.

Already for $n=20$, it becomes apparent in the right panel of
\reffig{comp-unc} that the uncertainty for $\Lumtot$ is dominated by
the Poisson uncertainty: both $p(Q \cond n, \alpha_0, \beta_0)$ and
$p(Q \cond n, \bml)$ yield a standard error of $\approx 0.19$. This is
necessarily larger that the standard error of
$p(\Lumtot \cond \lambda, \alpha_0, \beta_0)$ because the variance of

The case where all parameters are known is not attainable in practice
and only shown for reference. But the case where $\alpha, \beta$ are
fixed can be approximated easily if many packets from other bins can
be combined.

Note that $n=2$ is the minimum number of packets to perform
the binned analysis.
\fred{I was playing around in mathematica and it seems even for $n=1$ the posterior has a well defined mode. I don't if it is proper, though. In any case it has huge variance.}

\begin{figure}[ht]
  \centering
  \includegraphics[width=\textwidth]{comp_unc}
  \caption{Prediction of $\Lumtot$ for $n=0,2,20$ considering $\lambda, \alpha, \beta$ fixed (green), $\alpha, \beta$ fixed (red), or nothing fixed (blue).}
  \label{fig:comp-unc}
\end{figure}

\section{Conclusion} \label{sec:conclusion}

\begin{enumerate}
  \item Poisson uncertainty dominates
  \item can predict reliably for small $n$
\end{enumerate}

\section{Appendix} \label{sec:appendix}

\subsection{Moments of the luminosity prediction}\label{sec:predict-moments}
\fred{Shouldn't we use $n, \bml$ instead of $\bmy$ to be consistent?
  $\bmy$ would mean we use all samples from all bins}

While the integral in \refeq{ral} cannot be integrated analytically,
the integrand is unimodal and strongly peaked, so that the Laplace or
saddle-point approximation~\cite[Ch. 27]{mackay2003information} can be
applied. If only the value $p(\Lumtot{=}\lumtot \cond n,m_1,m_2)$ at the
total data luminosity $q$ is sought, then one can approximate it
directly. In addition, we can compute the moments of $\Lumtot$ through
the \emph{moment generating function} \cite{stuart1994kendall}
\begin{align}
  \lleq{rap}
  M(t \cond \bmy) = \int_0^{\infty} \rmdx{\Lumtot} p(\Lumtot \cond \bmy) e^{-t \Lumtot} \,
\end{align}
from which the moments are accessible by differentiation
\begin{align}
  \lleq{raq}
  \expect{\Lumtot^k} = (-1)^k \eval{\dv[k]{M(t \cond \bmy)}{t}}_{t=0}
\end{align}
Assuming all involved integrals are absolutely convergent, we can
exchange the order of integration and find
\begin{align}
  \lleq{rar}
  M(t \cond \bmy) &= \int \rmdx{\lambda} \rmdx{\mu} \rmdx{\sigma^2} p(\lambda \cond \bmy) p( \mu, \sigma^2 \cond \bmy)  f(t \cond \lambda, \mu, \sigma^2),
  \intertext{where }
  f\left(t \cond  \lambda, \mu, \sigma^2\right) &\equiv \int \rmdx{\Lumtot} \GaussianDist \left( \Lumtot \cond \lambda \mu, \lambda (\mu^2 + \sigma^2) \right) e^{-t \Lumtot}\\
  &= \frac{1}{2} \exp \qty(t/2 \lambda (t (\mu^2 + \sigma^2)- 2\mu)) \erfc \qty( \frac{\sqrt{\lambda}(t \qty(\mu^2 + \sigma^2) - \mu)}{\sqrt{2 \qty(\mu^2 + \sigma^2)}}).
\end{align}
This implies that the mean and variance of $\Lumtot$ require an integral over the three parameters
\begin{align}
  \lleq{ras}
  \expect{\Lumtot} &= \int \rmdx{\lambda} \rmdx{\mu} \rmdx{\sigma^2} p(\lambda \cond n) p( \mu, \sigma^2 \cond \bmy) \qty(-\eval{\dv{f(t \cond  \lambda, \mu, \sigma^2)}{t}}_{t=0})\\
  \variance{\Lumtot} &= \int \rmdx{\lambda} \rmdx{\mu} \rmdx{\sigma^2} p(\lambda \cond n) p( \mu, \sigma^2 \cond \bmy) \qty{\eval{\dv[2]{f(t \cond  \lambda, \mu, \sigma^2)}{t}}_{t=0} - \qty(\eval{\dv{f(t \cond  \lambda, \mu, \sigma^2)}{t}}_{t=0})^2} \, .
\end{align}
\fred{Advantage of this approach: no need to store all packets anymore, sum and sum of squares can be accumulated online $\Rightarrow$ reduced computation and storage requirements}

\bibliographystyle{plain}
\bibliography{references}

\end{document}

% Local Variables:
% compile-command:"rubber --pdf -W refs --synctex draft"
% End: