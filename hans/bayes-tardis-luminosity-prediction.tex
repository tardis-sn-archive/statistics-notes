%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Bayesian prediction of luminosity distribution based on one simulation run:
% An example from TARDIS
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[11pt]{article}
\usepackage[a4paper, left=20mm, right=20mm, top=25mm, bottom=25mm]{geometry}
\usepackage[numbers]{natbib}
%\usepackage{hyperref}

\usepackage[colorinlistoftodos]{todonotes} % Use 'disable' to remove todos in final version
\newcommand{\fred}[1]{\todo[inline]{#1}} %
\newcommand{\checked}{\todo[color=green,noline]{\checkmark}} %
\newcommand{\checkedl}{\todo[color=brown]{\checkmark}} %

%% CHANGE THIS DATE AS YOU MODIFY THE FILE
\newcommand{\vdate}{160114}
\pagestyle{myheadings}
\markboth{Version \vdate} {Version \vdate}
\parindent=0mm
\def\pp{\hfill\par \vspace{\baselineskip}}
%% \def\pp{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{bm,amssymb,amsfonts,amsmath}  % bold math symbols, AMStex
\usepackage{graphicx}    % standard graphics
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\newcommand{\lleq}[1]{\label{#1} }
% TO REMOVE THE LABEL LETTERS FROM THE EQUATION DISPLAY, COMMENT OUT THIS LINE:
\renewcommand{\lleq}[1]{\label{#1} {\scriptstyle {\rm (#1)}} \hspace*{2ex} }
% \renewcommand{\labelenumi}{\arabic{section}.\arabic{subsection}.\arabic{enumi}}
% \renewcommand{\thesubsubsection}{\arabic{subsubsection}}

\newcounter{mnumi}
\newenvironment{mnumerate}{
  \begin{list}{\arabic{mnumi}.}
    {\usecounter{mnumi} \setlength{\itemsep}{0pt}
      \setlength{\leftmargin}{3ex}
    }
  }
  {\end{list}}

\newenvironment{mtemize}{
  \begin{list}{$\bullet$}
    {\setlength{\itemsep}{0pt}
     \setlength{\leftmargin}{3ex}
    }
  }
  {\end{list}}

\newcommand{\hmod}  {{\mathcal{H}}}  % Model
\newcommand{\ldef}{\;{:}{=}\;}
\newcommand{\rdef}{\;{=}{:}\;}
\newcommand{\realnumbers}{\mathbb{R}}
\newcommand{\integers}{\mathbb{Z}}
\newcommand{\cond}{\,|\,}
\newcommand{\NBD}{\mathrm{NB}}
\newcommand{\var}{\mathrm{var}}

\newcommand{\smA}{{\scriptscriptstyle A}}
\newcommand{\smB}{{\scriptscriptstyle B}}
\newcommand{\smH}{{\scriptscriptstyle H}}
\newcommand{\smK}{{\scriptscriptstyle K}}
\newcommand{\smL}{{\scriptscriptstyle L}}
\newcommand{\smM}{{\scriptscriptstyle M}}
\newcommand{\smN}{{\scriptscriptstyle N}}
\newcommand{\smR}{{\scriptscriptstyle R}}

\newcommand{\smo}{{\scriptscriptstyle 1}}
\newcommand{\smt}{{\scriptscriptstyle 2}}
\newcommand{\smr}{{\scriptscriptstyle 3}}
\newcommand{\smf}{{\scriptscriptstyle 4}}
\newcommand{\smv}{{\scriptscriptstyle 5}}
\newcommand{\smx}{{\scriptscriptstyle 6}}
\newcommand{\sms}{{\scriptscriptstyle 7}}
\newcommand{\sme}{{\scriptscriptstyle 8}}
\newcommand{\smn}{{\scriptscriptstyle 9}}

\newcommand{\bma}{{\bm{a}}}
\newcommand{\bmb}{{\bm{b}}}
\newcommand{\bmc}{{\bm{c}}}
\newcommand{\bmd}{{\bm{d}}}
\newcommand{\bme}{{\bm{e}}}
\newcommand{\bmf}{{\bm{f}}}
\newcommand{\bmg}{{\bm{g}}}
\newcommand{\bmh}{{\bm{h}}}
\newcommand{\bmi}{{\bm{i}}}
\newcommand{\bmj}{{\bm{j}}}
\newcommand{\bmk}{{\bm{k}}}
\newcommand{\bml}{{\bm{\ell}}}
\newcommand{\bmm}{{\bm{m}}}
\newcommand{\bmn}{{\bm{n}}}
\newcommand{\bmo}{{\bm{o}}}
\newcommand{\bmp}{{\bm{p}}}
\newcommand{\bmq}{{\bm{q}}}
\newcommand{\bmr}{{\bm{r}}}
\newcommand{\bms}{{\bm{s}}}
\newcommand{\bmt}{{\bm{t}}}
\newcommand{\bmu}{{\bm{u}}}
\newcommand{\bmv}{{\bm{v}}}
\newcommand{\bmw}{{\bm{w}}}
\newcommand{\bmx}{{{\bm{x}}}}
\newcommand{\bmy}{{\bm{y}}}
\newcommand{\bmz}{{\bm{z}}}
\newcommand{\bmA}{{\bm{A}}}
\newcommand{\bmB}{{\bm{B}}}
\newcommand{\bmC}{{\bm{C}}}
\newcommand{\bmD}{{\bm{D}}}
\newcommand{\bmE}{{\bm{E}}}
\newcommand{\bmF}{{\bm{F}}}
\newcommand{\bmG}{{\bm{G}}}
\newcommand{\bmH}{{\bm{H}}}
\newcommand{\bmI}{{\bm{I}}}
\newcommand{\bmJ}{{\bm{J}}}
\newcommand{\bmK}{{\bm{K}}}
\newcommand{\bmL}{{\bm{L}}}
\newcommand{\bmM}{{\bm{M}}}
\newcommand{\bmN}{{\bm{N}}}
\newcommand{\bmO}{{\bm{O}}}
\newcommand{\bmP}{{\bm{P}}}
\newcommand{\bmQ}{{\bm{Q}}}
\newcommand{\bmR}{{\bm{R}}}
\newcommand{\bmS}{{\bm{S}}}
\newcommand{\bmT}{{\bm{T}}}
\newcommand{\bmU}{{\bm{U}}}
\newcommand{\bmV}{{\bm{V}}}
\newcommand{\bmW}{{\bm{W}}}
\newcommand{\bmX}{{\bm{X}}}
\newcommand{\bmY}{{\bm{Y}}}
\newcommand{\bmZ}{{\bm{Z}}}

\newcommand{\bmalpha}{{\bm{\alpha}}}
\newcommand{\bmbeta}{{\bm{\beta}}}
\newcommand{\bmchi}{{\bm{\chi}}}
\newcommand{\bmdelta}{{\bm{\delta}}}
\newcommand{\bmepsilon}{{\bm{\varepsilon}}}
\newcommand{\bmphi}{{\bm{\phi}}}
\newcommand{\bmgamma}{{\bm{\gamma}}}
\newcommand{\bmeta}{{\bm{\eta}}}
\newcommand{\bmiota}{{\bm{\iota}}}
\newcommand{\bmkappa}{{\bm{\kappa}}}
\newcommand{\bmlambda}{{\bm{\lambda}}}
\newcommand{\bmmu}{{\bm{\mu}}}
\newcommand{\bmnu}{{\bm{\nu}}}
\newcommand{\bmomega}{{\bm{\omega}}}
\newcommand{\bmpi}{{\bm{\pi}}}
\newcommand{\bmpsi}{{\bm{\psi}}}
\newcommand{\bmrho}{{\bm{\rho}}}
\newcommand{\bmsigma}{{\bm{\sigma}}}
\newcommand{\bmtau}{{\bm{\tau}}}
\newcommand{\bmtheta}{{\bm{\theta}}}
\newcommand{\bmupsilon}{{\bm{\upsilon}}}
\newcommand{\bmxi}{{\bm{\xi}}}
\newcommand{\bmzeta}{{\bm{\zeta}}}

\DeclareMathOperator{\GammaDist}{Gamma}
\newcommand{\refeq}[1]{Eq.~(\ref{#1})}
\newcommand{\lmax}{\ell_{\rm max}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\begin{center}
  \textbf{\Large Bayesian prediction of supernova luminosity  distributions}\\[8pt]
  \textbf{\Large based on simulation output: An example from TARDIS}\\[12pt]
\end{center}

\textbf{Abstract} TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introduction}

TODO: Wolfgang

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Bayesian prediction}

\textbf{HANS NOTE: I have just copied stuff in here which will be
  written up properly later.}
\begin{mtemize}
\item Generic definition of terms: data, parameter, variable
\item Define likelihood $p(x\cond\theta,\hmod)$
\item Product rule and sum rule (marginalisation)
\item Bayes' Theorem

\item Building a model $\hmod$ implies that we hypothesise both a
  likelihood $p(x\cond\bmtheta,\hmod)$ and a prior
  $p(\bmtheta\cond\hmod)$.

\item $p(D\cond\theta,\hmod)$ is the \textbf{likelihood} for $\theta$,
  given the data $D$. Note that, while the likelihood appears to be
  identical with the sampling distribution $p(D\cond\theta,\hmod)$
  both in notation and even in its functional form, these two
  quantities have a different purpose: The likelihood is a function of
  $\theta$ for fixed data $D$, while the forward probability is a
  function of $D$ for fixed $\theta$.

\item $p(\theta\cond\hmod)$ is the \textbf{prior probability} for
  $\theta$, before taking into account the information provided by the
  data $D$.

\item $p(D\cond\hmod)$ is sometimes called the \textbf{evidence}: it
  contains just the data, not any particular value of $\theta$. The
  only way to connect the ``evidence'' to the other probabilities is
  to expand it in terms of the sum rule, $p(D\cond\hmod) =
  \sum_{\theta'} p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)$. In
  this sense it can be thought of as the normalisation constant which
  ensures that the posterior is properly normalised.


\item $p(\theta\cond D,\hmod)$ is the \textbf{posterior probability}
  for $\theta$, after the data $D$ has been taken into account. Given
  a set of observations $D$, the $p(\theta\cond D,\hmod)$ gives the
  probability that $\theta$ takes on a particular value.

\item \texttt{COMMENT: the bracketed letters to the left of equations
    is a temporary aid to facilitate referencing. A change in
    definition of command} \verb=\lleq= \textbf{will make them revert
    to ordinary equation labels.}

\item \textbf{Bayes' Theorem}: In terms of these quantities, Bayes'
  theorem, the central theorem of inference is given by
  \begin{align}
    \lleq{bth}
    p(\theta\cond D,\hmod)
    &= \frac{p(D\cond\theta,\hmod)\;p(\theta\cond\hmod)}
    {p(D\cond\hmod)}
    = \frac{p(D\cond\theta,\hmod)\;p(\theta\cond\hmod)}
    {\int d\theta'\; p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)}
  \end{align}
  where the integral in the denominator is replaced by a sum if
  $\theta$ takes on discrete values.

\item Note that if the prior is uniform, it is independent of $\theta$
  and will cancel; we are then left with
  \begin{equation}
    \lleq{bthu}
    p(\theta\cond D,\hmod)
    = \frac{p(D\cond\theta,\hmod)} {\int d\theta'\; p(D\cond\theta',\hmod)}
  \end{equation}

\item \textbf{Prediction using Bayes' Theorem:} By contrast, the
  Bayesian prediction for $x_{n+1}$ makes full use of the information
  contained in previous data:
  \begin{align}
    p(x_{n+1}\cond D,\hmod)
    &= \int d\theta'\; p(x_{n+1}\cond\theta',D,\hmod)\;p(\theta'\cond
    D,\hmod)
  \end{align}
  i.e.\ all possible values of $\theta$ are taken into account. Using
  Bayes' theorem, this can be written also as
  \begin{align}
    p(x_{n+1}\cond D,\hmod)
    &= \frac{\int d\theta'\; p(x_{n+1}\cond\theta',D,\hmod)\;
      p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)}
    {\int d\theta'\; p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)}
  \end{align}
  which shows that the prediction for $x_{n+1}$ is a weighted average
  of $p(x_{n+1}\cond\theta',D,\hmod)$ with the weight given by
  \begin{align}
  \frac{p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)}
  {\int d\theta'\; p(D\cond\theta',\hmod)\;p(\theta'\cond\hmod)}
  &=
  \frac{p(D,\theta'\cond\hmod)}
  {\int d\theta'\; p(D,\theta'\cond\hmod)}
  \end{align}
\end{mtemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{TARDIS}

\subsection{Intro to those TARDIS aspects relevant to this paper}

TODO: Wolfgang (or in Introduction?)

\subsection{Definitions of relevant quantities}

\begin{mtemize}
\item Spectrum of luminosities in channels (or ``bins'')
  $b=0,1,\ldots,B$
\item The wavelength of channel $b$ is given by $\lambda_b =
  \lambda_0 + b\,\delta\lambda$, where $\delta\lambda$ is the bin width.

\item We treat the results of a single TARDIS run as \textit{data}
  $D$, i.e.\ fixed results which we do not question but accept as the
  true values.  Following a single run, the TARDIS ``data'' consists,
  for each bin $b$, of the number or ``multiplicity'' of packets and
  the individual packet luminosities,
\begin{align}
  \lleq{dfc}
  D &= \{n_0,\bml_0,n_1,\bml_1,\ldots,n_\smB,\bml_\smB\}
  \ =\ \{n_b,\bml_b\}_{b=0}^\smB
\end{align}
  \checked{}
with
\begin{align}
  \lleq{dfd}
  \bml_b &= (\ell_{b,1},\ell_{b,2},\ldots,\ell_{b,n_b})
\end{align}
  \checked{}
Given $D$, the total luminosity in bin $b$ is known,
\begin{align}
  \lleq{dfe}
  \ell_b = \sum_{j=1}^{n_b}\ell_{b,j}
\end{align}
  \checked{}
\end{mtemize}

\subsection{Data as realization of compound process}

While we treat the TARDIS output as ``data'' as discussed above, a
repeat of the simulation with a different Monte Carlo seed would
evidently yield different ``data'' $D'$ even when all other simulation
parameters remain the same. Clearly, $n_b$ is a specific realization
of a stochastic variable $N_b$ governed by a model pdf
$p(N_b)$. \fred{In our general case, $\sum_b n_b$ is an instance of a random variable because some packets leave the supernova, some don't. I guess in most applications the total number of events would be chosen and not random. We are fortunate that this difference does not matter because we consider bins independently. } Similarly, $\bml_b$ is a specific realization of a
collection of stochastic variables $\bmL_b
=\{L_{b,1},\ldots,L_{b,\smN_b}\}$, where the number $N_b$ of such
variables is itself stochastic. The relevant pdf for $\bmL_b$
therefore always presupposes knowledge of $N_b$, so we must work with
the conditional probability $p(\bmL_b\cond N_b)$. This situation is
captured mathematically as $N_b \sim p(N_b), \bmL_b \sim p(\bmL_b\cond
N_b)$ where $\sim$ means ``is by hypothesis distributed in this model
according to the pdf'', i.e.\
\begin{align*}
  (N_b,\bmL_b) &\ \sim\ p(N_b,\bmL_b\cond \bmphi_b,\lambda_b)
  = p(\bmL_b\cond N_b,\bmphi_b)\; p(N_b\cond \lambda_b),
\end{align*}
where $\bmphi_b$ is the set of parameters\fred{so $\bmphi$ includes random seed or not? Initially I thought so. But later on it becomes clear that you do not include it.} of the model pdf which
govern $\bmL_b$ and $\lambda_b$ is the parameter of the model pdf
governing $N_b$.  To simulate $(N_b,\bmL_b)$, we clearly first need to
generate one $N_b$, followed by the generation of $N$ packet
luminosities $\bmL_b$. This situation is termed a \textit{compound
  process}.

\subsection{Construction of the model hypothesis}

\begin{mtemize}
\item In the remainder of this paper, we assume that the packet
  multiplicities $N_b$ and total luminosities $\bmL_b$ in any given
  bin $b$ are statistically independent of those in other bins. \fred{We could motivate this in the usual way as the Poisson limit of the Binomial. The subtlety would then the be the random total number of packets (leaving the supernova).} This
  means that the joint probability over all bins factorises into
  individual probabilities for each bin,
  \begin{align}
    \lleq{bsi}
    p(N_0,\bmL_0\ldots,N_b,\bmL_b,\ldots,N_\smB,\bmL_\smB
    \cond \bmtheta,\bmphi,\bmlambda,\hmod)
    &= \prod_{b=0}^B p(N_b,\bmL_b\cond \bmtheta,\bmphi,\bmlambda,\hmod)
    % p(N_0,\ldots,N_b,\ldots,N_\smB \cond \bmtheta,\bmphi,\hmod)
    % &= \prod_{b=0}^B p(N_b\cond \bmtheta,\bmphi,\hmod)\\
    % p(L_0,\ldots,L_b,\ldots,L_\smB \cond N_0,\ldots,N_\smB,\bmtheta,\bmphi,\hmod)
    % &= \prod_{b=0}^B p(L_b\cond N_b,\bmtheta,\bmphi,\hmod)
  \end{align}

\item We omit the supernova parameters $\bmtheta$ from the notation
  below.

\item Based on the assumption of bin-bin statistical independence, we
  shall henceforth drop the bin index and do all our calculations for
  an individual bin $b$ only, writing for example $L_b \to L$,
  $\ell_{b,j} \to \ell_j$ etc. The total spectrum is then obtained by
  doing any given analysis or calculation for all bins.

\item Since we shall not be doing model comparison, we can for the
  moment drop the $\hmod$ from the probability notation.

\item The \textbf{model} which we construct consists of the above
  assumptions plus
  \begin{align*}
    \text{likelihood } &\ p(\bmL\cond N,\bmphi)\\
    \text{likelihood } &\ p(N\cond\lambda)\\
    \text{prior } &\ p(\bmphi)\\
    \text{prior } &\ p(\lambda)
  \end{align*}
  which are assumed to have the same functional form in each bin,
  while of course the parameter values will change from bin to bin.

\item We choose $N$ to follow a Poisson distribution with parameter
  $\lambda$ \fred{this choice linked to limit of Multinomial, see above}
  \begin{align}
    \lleq{pmc}
    p(N\cond\lambda) &= \frac{e^{-\lambda}\lambda^N}{N!}
    \qquad N = 0,1,\ldots,\infty,\quad \lambda>0
  \end{align}

\item We further assume that individual packets are generated
  independently,
  \begin{align}
    \lleq{pmda}
    p(\bmL\cond N,\bmphi) &= \prod_{j=1}^N p(L_j\cond \bmphi)
  \end{align}

\item The values of $\lambda$ and $\bmphi$ are unknown. It is the task
  of the analysis below to determine not just the single best values
  for these parameters, but to take into account every and all
  possible values.

\item Priors: metaparameter $a$ \fred{?}

\end{mtemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prediction based on individual packets}


\subsection{Predicting the packet multiplicity $N$ in a bin}

\texttt{COMMENT: The derivation below is deliberately slow; the
  intention being to give the novice an easy introduction into the
  usage of Bayes before we hit the more complicated calculations
  further below.}

We first show how to use Bayesian prediction to find the probability
distribution for $N$, the number of packets in a bin, given knowledge
of ``data'' in the form of TARDIS multiplicity $n$ for that bin.
Since $n$ is a specific
realization of $N$, it results from the same Poisson distribution
\begin{align}
  \lleq{pmcb}
   p(n\cond\lambda) &= \frac{e^{-\lambda}\lambda^n}{n!}
\end{align}
with the same value for $\lambda$ as in (\ref{pmc}).  We can hence use
Bayes' Theorem to find the posterior distribution for $\lambda$ given
data $n$ and the Poisson hypothesis
\begin{align}
  \lleq{pmd}
  p(\lambda\cond n) %
  &= \frac{p(n\cond\lambda)\,p(\lambda)} {p(n)}
  \ =\ \frac{p(n\cond\lambda)\,p(\lambda)} %
  {\int d\lambda\, p(n\cond\lambda)\,p(\lambda)}
\end{align}
to find the prediction of $N$ given $n$ taking into account all
possible values of $\lambda$,
\begin{align}
  \lleq{pme}
  p(N\cond n)
  &= \int d\lambda\,p(N,\lambda\cond n)
  \ =\ \int d\lambda\,p(N\cond\lambda, n)\,p(\lambda\cond n)
  \ =\ \frac{\int d\lambda\,p(N\cond\lambda)\,p(n\cond\lambda)\,p(\lambda)}
  {\int d\lambda\, p(n\cond\lambda)\,p(\lambda)}
\end{align}\checked{}
where $p(N\cond\lambda,n) = p(N\cond\lambda)$ is independent of $n$.
Taking an improper prior (see Appendix)\footnote{For a uniform prior,
  the case $a{=}0$, we require of course a maximum $\lambda_{\rm
    max}$. As long as $\lambda_{\rm max}$ is large enough, the
  integral can effectively be taken to infinity.}
\begin{align}
  \lleq{pmf}
  p(\lambda) = c\lambda^{-a} \qquad a = 0\;\text{or}\;1
\end{align}
and inserting this and (\ref{pmcb}) into (\ref{pme}), we obtain
for the evidence and prediction
\begin{align}
  \lleq{pmg}
  p(n) &= %
  \int_0^\infty d\lambda\, p(n\cond\lambda)\,p(\lambda)
  \ =\
  c \int_0^\infty d\lambda\, \frac{e^{-\lambda}\,\lambda^{n-a}}{n!}
  \ =\ c\frac{(n-a)!}{n!}
  \\
  \lleq{pmh}
  p(N\cond n) %
  &= \frac{n!}{(n-a)!} \int_0^\infty d\lambda\,
  \frac{e^{-2\lambda} \lambda^{N+n-a}}{N!\, n!}
  \ =\ \frac{(N+n-a)!}{N!\,(n-a)!\,2^{N+n+1-a}}
\end{align}
\fred{$a=1/2 \Leftrightarrow$ Jeffrey's prior would my choice. We then get a PÃ³lya distribution instead of Negative Binomial but nothing substantial changes. $$(pmh) \frac{\Gamma(N+n-a+1)}{N!\,\Gamma(n-a+1)\,2^{N+n+1-a}}$$ }
which we recognise as a Negative Binomial distribution (see appendix)
\fred{There are several definitions of the Negative Binomial, so we should specify which one we use. This one coincides with Wikipedia: $N$ successes given $k$ failures with probability of success $\rho$.}
with parameters $k=n+1-a$ and $\rho=(1/2)$
\begin{align}
  \lleq{pmhb}
  p(N\cond n) &= \NBD(N\cond k{=}n{+}1{-}a, \rho{=}\tfrac{1}{2})
  \ =\ \binom{N{+}n{-}a}{N}
  \left(\frac{1}{2}\right)^N \left(\frac{1}{2}\right)^{n+1-a}
\end{align}
We shall use the properties of the Negative Binomial in the
calculations below.  Note that the distribution of $N$ given $n$ is
independent of the packets $\bmL$ and $\bml$ by construction.

\subsection{Predicting the total luminosity $L$ in a bin}

\texttt{COMMENT: We may want to eliminate some intermediate steps in
  the derivations below in the final paper. Wolfgang: how much detail
  should be supplied in the paper?}

\subsubsection{Direct probability}

\fred{We should be more general: let's reserve $n$ for the number of
  packets observed in a bin. Suppose this is $n=1$. From that, we can
  hardly infer the luminosity distribution. Instead, we group the
  packets in larger bins such that there are at least $m \gtrsim 300$
  packets, and then assume the distribution doesn't change much with
  frequency.

  Hence we will learn $\bmphi$ from $m$ samples, and
  $p(\bmphi\cond N,n,\bml) \to p(\bmphi\cond N,m,\bml)$ and we can
  keep $\bml$ with the same meaning. So we replace $n \to m$ below
  except in $p(N \cond n$)}

Our goal of prediction of $L$ given the data $(n,\bml)$ is captured in
the probability $p(L\cond n,\bml)$ with all relevant parameters
integrated out. We first note from Eq.~(\ref{pmh}) that $p(N\cond
n,\bml) = p(N\cond n)$ does not depend on $\bml$. Furthermore, since
$L$ is fully determined by $(N,\bmL)$,
\begin{align}
  \lleq{pmjd}
  L &= \sum_{j=1}^N L_j
\end{align}
is does not depend directly on $(n,\bml)$, i.e.\
\begin{align}
  \lleq{pmi}
  p(L\cond N,\bmL,n,\bml)
  = p(L\cond N,\bmL)
  &= \delta(L - \textstyle\sum_{j=1}^N L_j).
\end{align}
\checked{}
Hence the prediction is given by
\begin{align}
  \lleq{pmj}
  p(L\cond n,\bml)
  &= \sum_{N=0}^\infty p(L\cond N,n,\bml)\,p(N\cond n,\bml) \\
  \lleq{pmjb}
  &= \sum_N p(N\cond n)\,\int d\bmL\, p(L\cond N,\bmL,n,\bml)\, p(\bmL\cond N,n,\bml)
  \\
  \lleq{pmjc}
  &= \sum_N p(N\cond n)\,\int d\bmL\,\delta(L - \textstyle\sum_{j=1}^N L_j)
  \, p(\bmL\cond N,n,\bml)
\end{align}
\checked{}
The dependence on the TARDIS ``data'' $(n,\bml)$ enters through
$p(\bmL\cond N,n,\bml)$ in terms of the parameters $\bmphi$ of
$p(\bmL\cond N,\bmphi)$,
\begin{align}
  \lleq{pmk}
  p(\bmL\cond N,n,\bml)
  &= \int d\bmphi\, p(\bmL\cond N,\bmphi,n,\bml)\,p(\bmphi\cond N,n,\bml) \\
  &= \int d\bmphi\, p(\bmL\cond N,\bmphi)\,p(\bmphi\cond N,n,\bml).
\end{align}
because the likelihood for $\bmL$ is fully determined by $N,\bmphi$
and hence independent of $(n,\bml)$.
%
Again using Bayes' Theorem (\ref{bth})
\begin{align}
  \lleq{pmkb}
  p(\bmphi\cond N,n,\bml) &=
  \frac{p(\bml\cond N,n,\bmphi)\,p(\bmphi\cond N,n)} %
  {\int d\bmphi\,p(\bml\cond N,n,\bmphi)\,p(\bmphi\cond N,n)} %
  % \ =\ \frac{p(\bml\cond N,n,\bmphi)\,p(\bmphi\cond N,n)} %
  % {p(\bml\cond n)} %
\end{align}
\fred{$p(\bmphi\cond N,n,\bml) = p(\bmphi\cond n,\bml)$ }
and using a multiplicity-independent prior for $\bmphi$
\begin{align}
  \lleq{pml}
  p(\bmphi\cond N,n) &= p(\bmphi)
\end{align}
we can write (\ref{pmk}) as
\begin{align}
  \lleq{pmn}
  p(\bmL\cond N,n,\bml)
  &= \int d\bmphi\, p(\bmL\cond N,\bmphi)\, %
  \frac{p(\bml\cond n,\bmphi)\,p(\bmphi)} %
  {\int d\bmphi\,p(\bml\cond n,\bmphi)\,p(\bmphi)} \\
  \lleq{pmo}
  &= \frac{1}  {p(\bml\cond n)} %
  \int d\bmphi\, p(\bmL\cond N,\bmphi)\, %
  p(\bml\cond n,\bmphi)\,p(\bmphi)
\end{align}
with $p(\bml\cond n) = \int d\bmphi\, p(\bml\cond
n,\bmphi)\,p(\bmphi)$ in the denominator.
Inserting this into (\ref{pmjc}), we obtain
\begin{equation}
  \lleq{pmp}
  \boxed{
  p(L\cond n,\bml)
  = \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\,\int d\bmL\,\delta(L - \textstyle\sum_j L_j)
  \displaystyle\int d\bmphi\,p(\bmL\cond N,\bmphi) \, p(\bml\cond n,\bmphi)
  \, p(\bmphi)
  }
\end{equation}
\checked{}
with
\begin{align}
  p(\bmL\cond N,\bmphi) &= \prod_{j=1}^N p(L_j\cond \bmphi) \\
  p(\bml\cond n,\bmphi) &= \prod_{j=1}^n p(\ell_j\cond \bmphi)
\end{align}
\fred{And $p(\cdot \cond \bmphi)$ is the same function in both equations.}
In other words: Once we have decided on a prior $p(\bmphi)$ and a
specific pdf to model the likelihood $p(L_j\cond \bmphi)$, we have a
prediction for $L$ which takes into account all possible $N$,
$\lambda$ and $\bmphi$.



\subsubsection{Moment generating function}
\label{sec:mgff}

\texttt{COMMENT Since the TARDIS ``data'' for $L_j$ is highly
  asymmetric, the model likelihood\\ $p(L_j\cond \bmphi)$ must also
  be. Hence we have to determine if a description based on moments is
  relevant! It could be that our numbers are a very bad description of
  the posterior.}
\\

It may be possible to calculate the prediction in Eq.~(\ref{pmp})
numerically, but it will be slow due to the multiple $d\bmL$
integral. A partial solution lies in at least finding simple
expressions for the moments and cumulants (mean, variance, skewness,
kurtosis etc) of $p(L\cond n,\ell)$. This can be accomplished by first
finding the moment generating function (mgf) (see Appendix) which in
the present case is also the Laplace transform. \fred{On Wikipedia,
  the MGF is defined as $\int_0^\infty dL\,p(L\cond
  n,\bml)\,e^{+tL}$. Then there are less minus signs to worry about}
For the mgf, the convolution is easily accomplished:
\begin{align}
  \lleq{mgc}
  M(t;L\cond n,\bml) &= M[t\cond p(L|n,\bml)]
  \ =\ \int_0^\infty dL\,p(L\cond n,\bml)\,e^{-tL} \\
  &= \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\,\int dL e^{-tL} \int d\bmL\,\delta(L - \textstyle\sum_j L_j)
  \displaystyle\int d\bmphi\,p(\bmL\cond N,\bmphi) \, p(\bml\cond n,\bmphi)
  \, p(\bmphi)\\
  &= \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\,\int d\bmphi\,
  \prod_{j=1}^N \left[ \int dL_j\,p(L_j\cond \bmphi)\,e^{-tL_j}\right]
   p(\bml\cond n,\bmphi) \, p(\bmphi)
\end{align}
The term in the square bracket is the moment generating function of
$p(L_j\cond \bmphi)$,
\begin{align}
  \lleq{mgcb}
  M(t; L_j\cond \bmphi) &=
  \int dL_j\,p(L_j\cond \bmphi)\,e^{-tL_j}
\end{align}
and since this is the same for all packets $L_j$, we obtain
\begin{equation}
  \lleq{mgd}
  \boxed{
  M(t;L\cond n,\bml) %
  = \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\,\int d\bmphi\,
  M(t; L_j\cond \bmphi)^N
   p(\bml\cond n,\bmphi) \, p(\bmphi)
   }
 \end{equation}
 \checked{}
As shown in Appendix ***, order-$q$ moments (expectation values of
$L_j^q$) of $p(L\cond n,\bml)$ can be found by taking successive
derivatives with respect to $t$,
\begin{align}
  \lleq{mge}
  E[L^q\cond n,\bml]
  &= \left(\frac{-\partial}{\partial t}\right)^q M(t;L\cond n,\bml)\biggr|_{t=0}
  \qquad q = 1,2,3\ldots
\end{align}
from which we can find the mean, variance, skewness etc of
$p(L|n,\bml)$ without knowing its form explicitly. To do so, we need
to find successive derivatives of $\left[ M(t; L_j\cond \bmphi)
\right]^N$ appearing in (\ref{mgd}). Defining the moments of
$p(L_j\cond\bmphi)$ as
\begin{align}
  \lleq{mgf}
  \mu_q(\bmphi) &= \int dL_j\,p(L_j\cond\bmphi)\,L_j^q \qquad q = 1,2,\ldots,
\end{align}
we find
\begin{align}
  \lleq{mgg}
  \left(\frac{-\partial}{\partial t}\right) %
  M(t; L_j\cond \bmphi)^N\biggr|_{t=0}
  &= N \mu_1(\bmphi)\\
  \lleq{mgh}
  \left(\frac{-\partial}{\partial t}\right)^2 %
  M(t; L_j\cond \bmphi)^N\biggr|_{t=0}
  &= N^{\underline{2}}\mu_1^2(\bmphi) + N \mu_2(\bmphi) \\
  \lleq{mgi}
  \left(\frac{-\partial}{\partial t}\right)^3 %
  M(t; L_j\cond \bmphi)^N\biggr|_{t=0}
  &= N^{\underline{3}}\mu_1^3(\bmphi) + 3N^{\underline{2}}\mu_1(\bmphi)\mu_2(\bmphi)
  + N\mu_3(\bmphi)
\end{align}
\checked{}
where
\begin{align}
  \lleq{mgj}
  N^{\underline q} &= N(N-1)\cdots (N-q+1) = \frac{N!}{(N-q)!}
\end{align}
is the $q$th falling power of $N$. Successively inserting these and
(\ref{mgd}) into (\ref{mge}), we firstly get for the mean of $L$,
given $(n,\bml)$
\begin{align}
  \lleq{mgk}
  E[L\cond n,\bml]
  &= \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\,N \int d\bmphi\, \mu_1(\bmphi) \,
  p(\bml\cond n,\bmphi) \, p(\bmphi).
\end{align}
Given that $p(N\cond n)$ is a Negative Binomial pdf, the sum is trivially
found to be
\begin{align}
  \lleq{mgl}
  \sum_N p(N\cond n)\,N &= n\frac{\tfrac{1}{2}}{1-\tfrac{1}{2}} = n
\end{align}
\fred{The first moment under NB is $\frac{\rho k}{1-\rho}$, resulting in $n-a+1$. Here and below you seem to set $a=1$ implicitly. I want to carry the $a$ along.}
so that
\begin{align}
  \lleq{mgm}
  E[L\cond n,\bml]
  &= \frac{n}{p(\bml\cond n)}
  \int d\bmphi\, \mu_1(\bmphi) \, p(\bml\cond n,\bmphi) \, p(\bmphi)
\end{align}
Since the integral is an average of $\mu_1(\bmphi)$ weighted by
$p(\bmphi\cond n, \bml)$, let us give it a name. Given a function
$f(\bmphi)$, its ``$\bmphi$-expectation value'' is defined as
\begin{align}
  \lleq{mgn}
  E_\phi[ f ] &= \int d\bmphi\, f(\bmphi) \, p(\bml\cond n,\bmphi) \, p(\bmphi)
\end{align}
in terms of which
\begin{align}
  % \lleq{mgpa}
  % p(\bml\cond n) &= E_\phi[\mu_0],\\
  %%%%%%%
  \lleq{mgo}
  E[L\cond n,\bml]
  &= n\, \frac{E_\phi[\mu_1]}{p(\bml\cond n)}
  \ =\ n\,\frac{E_\phi[\mu_1]}{E_\phi[\mu_0]}
\end{align}
\fred{$n \to n-a+1$}
On taking the second derivative and using
\begin{align}
  \lleq{mgp}
  \sum_N p(N\cond n)\,N^2
  &= \frac{n(n+1)\left(\tfrac{1}{2}\right)^2}{\left(1-\tfrac{1}{2}\right)^2}
  = n(n+1)
\end{align}
\fred{I disagree. The 2nd raw moment is $\frac{k \rho (1 + k
    \rho)}{(1-\rho)^2}$ (from Mathworld with $\rho=q,1-\rho=p, k=r$
  and manually derived from the negative binomial MGF from
  Wikipedia). With $k=n-a+1$ and $\rho=1/2$, I get $(n-a+1)(n-a+3)$}
the second moment of $p(L\cond n,\bml)$ is
\begin{align}
  \lleq{mgq}
  E[L^2\cond n,\bml]
  &= \frac{1}{p(\bml\cond n)}
  \sum_N p(N\cond n)\, \int d\bmphi\,
  \left[ N^{\underline{2}}\mu_1^2(\bmphi) + N \mu_2(\bmphi)\right]
  p(\bml\cond n,\bmphi) \, p(\bmphi) \\
  &= n^2\,\frac{E_\phi[\mu_1^2]}{E_\phi[1]}
  + n \frac{E_\phi[\mu_2]}{E_\phi[1]}
\end{align}
\fred{I have
 $$
  \frac{n-a+1}{E_\phi[1]} \left( (n-a+2)E_\phi[\mu_1^2] + E_\phi[\mu_2]\right)
$$
}
so that the variance is
\begin{align}
  \lleq{mgr}
  \var(L\cond n,\bml)
  &= n^2\, \frac{E_\phi[\mu_1^2]\,E_\phi[1] -  E_\phi[\mu_1]^2}
  {E_\phi[1]^2}
  + n\, \frac{E_\phi[\mu_2]}{E_\phi[1]}
\end{align}
It is tempting to argue that the $n^2$ term is much larger than the
$n$-order term. This depends, however, on the sizes of the respective
expectation values: no a priori assumptions can be made here.
\fred{I have the more complicated
  $$
    \var(L\cond n,\bml) = \frac{n-a+1}{E_\phi[1]^2} \left( (n-a+2)E_\phi[\mu_1^2] E_\phi[1] -(n-a+1) E_\phi[\mu_1]^2 + E_\phi[\mu_2] E_\phi[1]\right)
  $$
}

\subsection{Third-order: skewness}

TODO Hans

This may be necessary because $p(L_j\cond\bmphi)$ is highly
asymmetric, while the pdf of the sum $L$ may be less so.

\fred{I agree but what would we do if we knew the skewness? For the fit, we need to evaluate the likelihood, so we'd have to build a function my moment matching. Of course with mean and variance alone, we'd use the Gaussian. But this just reduces to the central limit theorem.}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Modeling the TARDIS packet distribution}

TODO: All (numerics and analytics)

\begin{mtemize}
\item Find the most suitable likelihood for $p(L_j\cond \bmphi)$

\item Determine how far we can integrate analytically over the
parameters

\item How stable are the results to perturbations?

\item \texttt{COMMENT Since $p(L_j\cond \bmphi)$ is highly asymmetric,
    we have to determine if a description based on moments is
    relevant! It could be that our numbers are a very bad description
    of the posterior.}

\end{mtemize}

\fred{Yes but at least if a rather large number of events is in the
  bin, this will be good enough}

\subsection{Results for Beta distribution likelihood}

The Beta probability density function (pdf) is
\begin{align}
  \lleq{btc}
  p(x\cond a,b) &= \frac{x^{a-1}(1-x)^{b-1}}{B(a,b)} \qquad
  0 < x < 1, \quad a,b > 0.
\end{align}
where $\bmphi = (a,b)$ are the parameters and
\begin{align}
  \lleq{btd}
  B(a,b) &= \frac{\Gamma(a)\,\Gamma(b)}{\Gamma(a+b)}
\end{align}
is the beta function.
%
The form of the TARDIS single-packet distribution suggests that this
is appropriate; the strong asymmetry in the TARDIS distribution leads
us to expect most likely parameter values of $b \ll a$ so that the mode
is close to $x=1$.
%
The variable $x$ is related to packet luminosities simply by
$x = L_j/L_{\rm max}$ where $L_{\rm max}$ is the largest luminosity
in the packets i.e.\ $0 < L_j < L_{\rm max}$.
%
(Remember we are considering a single bin $b$ only, and only a single
TARDIS run.)
%
Given that we want to find the $\phi$-moments of Section \ref{sec:mgff}
in terms of TARDIS simulation ``data'', we define in this section
\begin{align}
  x_j &= \ell_j/\ell_{\rm max}
\end{align}
while $x$ without subscript is used for the generic moment
calculations. The same quantities could, of course, be used in terms
of the generic variable $x_j = L_j/L_{\rm max}$. In terms of the
original ``data'' packets, the beta distribution reads
\begin{align}
  \lleq{bte}
  p(\bml\cond \ell_{\rm max},n,a,b)
  &= \prod_{j=1}^n p(\ell_j\cond \ell_{\rm max},a,b) \\
  \lleq{btf}
  p(\ell_j\cond \ell_{\rm max},a,b)
  &= \frac{1}{\ell_{\rm max}\,B(a,b)}
  \,\left(\frac{\ell_j}{\ell_{\rm max}}\right)^{a-1}
    \left(1-\frac{\ell_j}{\ell_{\rm max}}\right)^{b-1}
  \ =\ \frac{1}{B(a,b)}
  \frac{\ell_j^{a-1}\,(\ell_{\rm max}-\ell_j)^{b-1}}
  {\ell_{\rm max}^{a+b-1}}
\end{align}
\checked{}
equivalent to the form (\ref{btc}) with $x\to x_j$.
% \begin{align}
%   p(x_j\cond a,b) &= \frac{1}{B(a,b)} x_j^{a-1} (1-x_j)^{b-1}
% \end{align}
NOTE: If $\ell_{\rm max}$ changes significantly from TARDIS run to
run, then the probability calculation may have to be changed from
$p(\bml\cond \ell_{\rm max},n,a,b)$ to\\
$p(\bml\cond \ell_{\rm max},n,a,b)\,p(\ell_{\rm max}\cond
n,a,b)$. However we are supposed to deal with ONE TARDIS run, so that
$\ell_{\rm max}$ is known and fixed.
\fred{Rather: we approximate $p(\ell_{\rm max} \cond n, a,b)$ by $\delta(\max \ell_{b,j})$. This is OK is $\sum_b n_b$ is large. For numerical stability, we want to ensure that $x<1$, so we take the largest $\ell_{b,j}$ and multiply by $1 +\varepsilon, \varepsilon = 10^{-7}$}

The moments of the beta distibution are simple,
\begin{align}
  \lleq{btg}
  \mu_q(a,b) &= \int dx\,x^q\,p(x\cond a,b)
  = \frac{(a)_q}{(a+b)_q}
  = \frac{a(a+1)\cdots(a+q-1)} {(a+b)(a+b+1)\cdots(a+b+q-1)}
  \qquad q = 0,1,2,\ldots
\end{align}
with $(a)_b$ the Pochhammer symbol.
%
\\
\fred{Formula ok but to be consistent with the notation used above, we should either use the rising factorial $a^{\overline{b}}$ here or $(a)^b$ above}

\textbf{Calculations for a uniform discrete prior}

The prior is as yet unspecified. Since there is no known conjugate
prior for the beta distribution,\fred{yes it is known. But it not
  useful because the normalization can't be calculated analytically;
  see
  http://mathoverflow.net/questions/20399/conjugate-prior-of-the-dirichlet-distribution}
we resort to a simpler prior in terms of discrete values for $a$ and
$b$ to simplify the expressions. In principle, these can be any
rational numbers, but because as stated the value of $b$ is expected
to be around 30--60, we let $a$ and $b$ be integers only, $a,b, =
1,2,3,\ldots$. \fred{I don't understand this argument} Furthermore,
since Eqs.~(\ref{mgo}) and (\ref{mgr}) always involve \textbf{ratios}
of $\phi$-expectation values, we can use a uniform improper prior,
\begin{align}
  p(a,b) &= C\qquad a,b = 1,2,\ldots,\infty
\end{align}
with $C$ the improper constant which will cancel in the ratio.
% With
% this prior, the $\phi$-expectation values become
% \begin{align}
%   E_\phi[\mu_q] &= C \sum_{a=1}^\infty \sum_{b=1}^\infty
%   \mu_q(a,b)\,\prod_j \frac{x_j^{a-1}(1-x_j)^{b-1}}{B(a,b)}
% \end{align}
We now calculate $\phi$-expectation values for this case, namely
\begin{align}
  \lleq{bthi}
  E_\phi[\mu_q(a,b)] &= \sum_{a,b}\mu_q(a,b)\,p(\bml\cond \ell_{\rm max}, n,a,b)
  \,p(a,b)
  \\
  % = C \sum_{a,b=1}^\infty \mu_q(a,b)\,p(\bml\cond \ell_{\rm max}, n,a,b)\\
  \lleq{bti}
  &= C\sum_{a,b} \frac{(a)_q}{(a+b)_q}
  \frac{1}{B(a,b)} \prod_{j=1}^n \left(x_j^{a-1}(1-x_j)^{b-1}\right)
\end{align}
\checked{}
The product can be rewritten as
\begin{align}
  \lleq{btj}
  \prod_{j=1}^n \left(x_j^{a-1}(1-x_j)^{b-1}\right)
  &= \left(\prod_j x_j\right)^{a-1}\left(\prod_j(1-x_j)\right)^{b-1}
\end{align}
\fred{This equation can be skipped}
and defining
\begin{align}
  \lleq{btk}
  y &= \prod_{j=1}^n x_j \qquad z = \prod_{j=1}^n(1-x_j)
\end{align}
\checked{}
which are data quantities independent of any model or model
parameters, we get
\begin{align}
  \lleq{btl}
  E_\phi[\mu_q(a,b)] &= C\sum_{a,b} \frac{(a)_q}{(a+b)_q}
  \frac{y^{a-1}z^{b-1}}{B(a,b)}
\end{align}
\checked{}
With the help of mathematica we find
\begin{align}
  \lleq{btm}
  E_\phi[\mu_0] &= C\sum_{a,b=1}^\infty \frac{y^{a-1}z^{b-1}}{B(a,b)}
  = \frac{C}{(1 - y - z)^2}
  \\
  E_\phi[\mu_1] &= C\sum_{a,b} \frac{a}{(a+b)}
  \frac{y^{a-1}z^{b-1}}{B(a,b)}
  \nonumber\\
  \lleq{btn}
  &= \frac{C}{(y + z)^3}
  \left[
  \frac{(y + z) [2 y^2 + (z-y)(z-1)]}
  {(1 - y - z)^2 }
   + (z-y) \ln(1 - y - z)
   \right]
%   \\
%   E_\phi[\mu_2(a,b)] &= C\sum_{a,b} \frac{a(a+1)}{(a+b)(a+b+1)}
%   \frac{(1-y)^{a-1}(1-z)^{b-1}}{B(a,b)}
%   \\
%   E_\phi[\mu_1^2(a,b)] &= C\sum_{a,b} \left(\frac{a}{a+b}\right)^2
%   \frac{(1-y)^{a-1}(1-z)^{b-1}}{B(a,b)}
 \end{align}
 \checked{}
Taking ratios, we obtain
\begin{align}
  \lleq{bto}
  \frac{E_\phi[\mu_1(a,b)]} {E_\phi[\mu_0(a,b)]}
  &= \frac{(y + z) [2 y^2 +(z- y) (1 - z)] + (z-y) (1 - y - z)^2 \ln(1 - y - z)}
  {(y + z)^3}
\end{align}
and so
\begin{align}
  \lleq{btp}
  E[L\cond n,\bml] &= n \left[
  \frac{(y + z) [2 y^2 +(z- y) (1 - z)] + (z-y) (1 - y - z)^2 \ln(1 - y - z)}
  {(y + z)^3}\right]
\end{align}
is completely determined by the data $(n,\bml)$.  Similarly
\begin{align}
  E_\phi[\mu_2] &= C\sum_{a,b} \frac{a(a+1)}{(a+b)(a+b+1)}
  \frac{y^{a-1}z^{b-1}}{B(a,b)}
  \nonumber\\
  \lleq{btq} &= \frac{-1}{(1-y-z)^2 (y+z)^5} %
  \nonumber\\
  &\times\Bigl\{ 2 \Bigl[2 y^2(z+1) + yz(z-8) - (z-2) z^2\Bigr]
  (y+z-1)^2 \ln(-y-z+1)
  \nonumber\\
  &+ (y+z) \Bigl[
    y^4 - 2 y^3 (5z+3) + y^2[(22-19 z) z+4] - 4 yz (z-4) (z-1) + 4z^2 (z-1)^2
           \Bigr] \Bigr\}
  % \nonumber\\
  % &= -(1/((y+z-1)^2 (y+z)^5)) %
  % \nonumber\\
  % &\times \Bigl\{
  %   -4 y^3 + 6 y^4 - y^5 + 12 y^2 z - 16 y^3 z + 9 y^4 z + 12 y z^2 -
  %   42 y^2 z^2 + 29 y^3 z^2 - 4 z^3 - 12 y z^3 + 23 y^2 z^3 + 8 z^4 -
  %   4 z^5 \nonumber\\
  %   &+ \ln[1 - y - z] \Bigl[
  %   -4 y^2 + 8 y^3 - 4 y^4 + 16 y z - 28 y^2 z + 16 y^3 z -
  %     4 y^4 z - 4 z^2 - 26 y z^2 + 36 y^2 z^2 - 10 y^3 z^2 + 10 z^3 +
  %     8 y z^3 - 6 y^2 z^3 - 8 z^4 + 2 y z^4 + 2 z^5
  %     \Bigr]
  %   \Bigr\}
\end{align}
and
\begin{align}
  E_\phi[\mu_1^2] &= C\sum_{a,b} \frac{a^2}{(a+b)^2}
  \frac{y^{a-1}z^{b-1}}{B(a,b)}
  \nonumber\\
  \lleq{btr} %
  &= \frac{1}{y (y - z)^3}
  \Bigl\{
  (y - z) (-y + z + 2 y \ln[y]) + y (-2 + y + z)
  (\mathrm{PolyLog}[2, 1 - y] - \mathrm{PolyLog}[2, 1 - z])
  \Bigr\}
\end{align}
These have to be combined with the above to find the variance of
Eq.~(\ref{mgr}). The hope is that the complicated mathematical
expressions simplify. Either way, the variance is also a closed
solution in terms of $n$, $y$ and $z$.

\subsection{Results for the Gamma-distribution likelihood}

The Gamma distribution is with shape parameter $\alpha$ and rate
parameter $\beta$ is defined for $x \geq 0$ as
\begin{align}
  \lleq{gma}
  \GammaDist(x | \alpha, \beta) = \frac{\beta^\alpha}{\Gamma(\alpha)} x^{\alpha-1} e^{-\beta x} .
\end{align}
$\GammaDist(x \cond \alpha, \beta)$ is defined for $x \in [0, \infty]$
whereas the luminosity of real packets is bounded from above and has a
peak away from the bound. In order to describe the luminosity
distribution of real packets through $\GammaDist(\alpha, \beta)$, we
have to transform the luminosities. To this end, we first take the
largest luminosity from the entire frequency range and increase it
slightly to avoid numerical difficulties at the end point
\begin{align}
  \lleq{gme}
  \lmax \equiv (1+\epsilon) \times \max_j \ell_j,
\end{align}
where $ \epsilon=10^{-6}$.
We assume the total number of samples across all bins is large, hence
the effect of considering $\lmax$ a constant independent of the data
is negligible. We then transform the samples as
\begin{align}
  \lleq{gmf}
  x_j \equiv 1- \frac{\ell_j}{\lmax} > 0
\end{align}
and similarly for $L \to X$, then we infer $P(X \cond n, \bmx,
m)$. Only in the end, we transform back to a probability density in
$L$ via
\begin{align}
  \lleq{gmg}
  p(L  \cond n, \bml, m) = p(X  \cond n, \bmx, m) \frac{1}{\lmax}
\end{align}
While not obvious at first, a major numerical simplification arises
because of the $L \to X$ transformation. Rewriting the master
equation~(\ref{pmp}) formally as
\begin{align}
  \lleq{gmh}
  p(L  \cond n, \bml, m) = \sum_{N} p(N \cond n) p(L \cond N, \bml, m)
\end{align}
we see that the $ p(L \cond n, \bml, m)$ is a mixture density. If the
distribution of a single luminosity, $p(L \cond N=1, \bml, m)$ is
narrow in the sense that the standard deviation relative to the mean
is at the percent level, then the mixture density has one mode for
each $N$ and all modes are clearly separated. The transformation $L
\to x$ effects a strong overlap between adjacent $p(X \cond N, \bmx,
m)$ and leads to a unimodal $p(X \cond n, \bmx, m)$.
\fred{Show a plot: from hedgehog to single peak}

For our purposes, the most useful property of the Gamma
distribution is infinite divisibility; i.e., the sum of $N$ Gamma
variates is again a Gamma variate
\begin{align}
  \lleq{gmb}
  X_i \sim \GammaDist(\alpha, \beta) \Rightarrow \sum_{i=1}^N X_i \sim \GammaDist(N \alpha, \beta).
\end{align}
With $\bmphi = (\alpha, \beta)$, \refeq{gmb} is equivalent to
\begin{align}
  \lleq{gmc}
 p(X \cond N, \bmphi) &= \int d\bmX\,\delta(X - \textstyle\sum_j X_j)
  \,p(\bmX\cond N,\bmphi) = \GammaDist(X \cond N \alpha, \beta)
\end{align}
and this implies that the master equation~(\ref{pmp}) considerably simplifies as
\begin{align}
  \lleq{gmd}
    p(X\cond n,\bmx,m)
  = \frac{1}{p(\bmx\cond m)}
  \sum_N p(N\cond n)\, \int d\bmphi\, p(X \cond N, \bmphi)\, p(\bmx\cond m,\bmphi)
  \, p(\bmphi).
\end{align}

To explicitly evaluate \refeq{gmd}, we have to overcome the remaining
two difficulties: the sum over $N$ and the integral over
$\bmphi$. Regarding the latter, there is a conjugate prior but
unfortunately the normalization constant can not be evaluated in
closed form. It is thus of no use for us, and we turn to a
noninformative prior. The authors of \cite{moala2013bayesian} present
an overview of possible choices and conclude that the point estimate
given by the mode of the posterior is nearly insensitive to the prior
choice for 30 samples. We require in addition that the posterior is
narrow enough so we group samples in bins to have at least $m=150$
samples in each bin. The following approximations can be used in
different regions of the parameter space spanned by $n$ and $m$.

\begin{enumerate}
  \item For $m$ large, the posterior $p(\bmphi \cond m, \bmx) =
  p(\bmx\cond m,\bmphi) \, p(\bmphi) / p(\bmx \cond m)$ approaches a
  multivariate Gaussian with variance tending to zero as $1/m$ in the
  limit $m \to \infty$.  \fred{check} We can then assume the
  parameters $\bmphi$ known and fix them at the posterior mode
  $\bmphi^*$; i.e., $p(\bmphi \cond m, \bmx) \to \delta(\bmphi -
  \bmphi^*)$.
  \item For $n$ large, the Negative Binomial $p(N \cond n)$ approaches
  a Gaussian, defining a region around $N=n$ with the dominant
  contribution to the sum over $N$ in \refeq{gmd}. The central limit
  theorem states that for large $N$, $P(X \cond N, \bmphi) \approx
  \mathcal{N}(X \cond N \, E_\bmphi[X_j], N \, V_\bmphi[X_j])$. For
  $m$ large enough, we could then plug in the sample-mean and
  sample-variance estimates and bypass the assumption of a Gamma
  distribution altogether.
\end{enumerate}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Prediction based on packet sum}

TODO: Hans. The calculations have already been done. The question is
whether we even want to bother with them since data in the form of
packet sums is less information than the vector of individual packets
as used above.
\fred{No, we don't want to bother.}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Results}

In Figure~\ref{fig:beta} is a fit of the Beta distribution to 3000
samples from a TARDIS run with frequencies in the range $[1.367,
1.435] \times 10^{38}$ Hz. A characteristic feature seen in many such
similar fits is that the Beta distribution does not give a perfect
fit: it undershoots around the mode and overshoots around
$x=0.96$. The reason may be that the Beta itself is not flexible
enough to describe the data, or the distribution is simply not
constant in frequency, so we should choose smaller bins or less
samples. We don't expect 3000 samples in every bin, so what is the
minimum number of samples needed to work robustly? The first numerical
challenge is how to normalize the luminosities, I take the maximum
across all samples (not just one bin) and multiply by $10^{-6}$ to
avoid $x=1$ for any sample. For too few samples, the fit returns a pdf
that peaks at the boundary $x=1$, this is not what we want. From trial
and error, 300 samples is a solid minimum to get a robust fit if
parameters are estimated via maximum likelihood. The method-of-moment
estimator has the advantage of not requiring an optimization algorithm
but for some bins it gives a solution peaking at $x=1$ even for 800
samples so it's not robust.

\begin{figure}[h]
  \centering
  \includegraphics[width=0.48\textwidth]{beta-300}
  \includegraphics[width=0.48\textwidth]{beta-3000}
  \caption{Beta distribution fit. The samples are binned in the
    histogram. The blue solid curve is a kernel-density estimate, the
    red dashed curve is the Beta distribution. left: 300 samples,
    right: 3000 samples}
  \label{fig:beta}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Discussion and Conclusions}

TODO: All

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section*{Acknowledgements}

This work was supported in part by the DFG cluster of excellence
``Origin and Structure of the Universe'' (www.universe-cluster.de) and
the National Research Foundation of South Africa, etc etc.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix: Relevant distributions}

TODO: Hans

\subsection{Product rule and sum rule}

\subsection{Poisson}

\subsection{Negative Binomial}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Appendix: Generating functions}

TODO: Hans
$\pi(\alpha, \beta) \propto \left( \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha) \Gamma(\beta)} \right)^{\lambda +1} (x x_0)^{\alpha-1} \left(y_0 (1-x) \right)^{\beta-1}$

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \section{}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\bibliographystyle{plain}
\bibliography{references}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
